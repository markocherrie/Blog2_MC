<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Analysis on Mark Cherrie</title>
    <link>http://www.markcherrie.net/categories/data-analysis/</link>
    <description>Recent content in Data Analysis on Mark Cherrie</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2020 Mark Cherrie</copyright>
    <lastBuildDate>Thu, 17 Jan 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="/categories/data-analysis/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>How to build a structural equation model in Lavaan</title>
      <link>http://www.markcherrie.net/post/structural-equation-modelling-in-r/</link>
      <pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.markcherrie.net/post/structural-equation-modelling-in-r/</guid>
      <description>&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;I went on a &lt;a href=&#34;https://www.psychometrics.cam.ac.uk/trainingworkshops/structural-equation-modelling-in-r-4-day-course&#34;&gt;course&lt;/a&gt; in Cambridge over the summer of 2018. This was to get me up to speed on structural equation modelling (SEM), which has a lot of potential applications in scenarios where the pathways between measured and unmeasured variables are the central focus of the research question.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-sem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is SEM?&lt;/h2&gt;
&lt;p&gt;SEM is a mixture of confirmatory factor analysis (CFA) and path analysis. Another way to describe that, is that you have a measurement part and a structural part. They relate to two questions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How can we measure latent concepts?&lt;/li&gt;
&lt;li&gt;What is the relationship between the variables in our model?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The key point to structural equation modelling is that is splits measurement error and the correlation between the residuals from the structural coefficients. I recommend the Lavaan &lt;a href=&#34;http://lavaan.ugent.be/tutorial/est.html&#34;&gt;tutorial&lt;/a&gt;; below is my workflow for building a mediation/moderation model in Lavaan.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lavaan-syntax&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lavaan syntax&lt;/h2&gt;
&lt;p&gt;First of all the syntax for Lavaan models is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;~ Define regression formula&lt;/li&gt;
&lt;li&gt;~~ Define correlated residual variances (two observed variables)&lt;/li&gt;
&lt;li&gt;=~ Define latent variable&lt;/li&gt;
&lt;li&gt;:= Define effect (i.e. indirect or total)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Remember, you’ll need to define the model in speech marks and then use it as the model argument in the lavaan functions: cfa and sem.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;This example is on understanding the pathways to smoking in adolescents. The density of tobacco retailers in an adolescents’ residential neighbourhood predicts propensity to smoke; the greater the density of retailers the more likely smoking is &lt;a href=&#34;https://tobaccocontrol.bmj.com/content/25/1/75.info&#34;&gt;(Shortt et al., 2014)&lt;/a&gt;. However we know that density also predicts whether a family member will smoke and that if a family member is a smoker, the adolescent is more likely to smoke. Also, we know that the relationship between density of retailer and smoking is stronger when the individual has a lower socioeconomic status or lives in an area of higher deprivation &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/15965140&#34;&gt;(Chuang et al., 2005)&lt;/a&gt;. How can we model these latent contructs (family smoking) and complex relationships? This hypothetical dataset called ‘smokingdata’ has the following variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;density: continuous density of tobacco retailers&lt;/li&gt;
&lt;li&gt;smoking: binary smoking status&lt;/li&gt;
&lt;li&gt;family: family smoking latent variable&lt;/li&gt;
&lt;li&gt;brother: measured variable on smoking status in brother&lt;/li&gt;
&lt;li&gt;mother: measured variable on smoking status in mother&lt;/li&gt;
&lt;li&gt;sister: measured variable on smoking status in sister&lt;/li&gt;
&lt;li&gt;father: measured variable on smoking status in father&lt;/li&gt;
&lt;li&gt;deprivation: binary variable of deprivation in local neighbourhood&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;construct-latent-variables-using-cfa&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Construct latent variables using CFA&lt;/h2&gt;
&lt;p&gt;The first stage is to identify the latent constructs and the variables, which will be used to predict it (3 variables is minimum). Compare the model fit (use ‘fit.measures=T’) of several models and with what you expected given your reading of the literature.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lavaan)
cfa.model &amp;lt;- 
           &amp;#39; 
           # Measurement model
             family  =~ brother + mother + father + sister
           &amp;#39;
fit &amp;lt;- cfa(cfa.model, data = smokingdata, ordered=c(&amp;quot;brother&amp;quot;,&amp;quot;mother&amp;quot;,&amp;quot;father&amp;quot;,&amp;quot;sister&amp;quot;))
summary(fit, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mediation-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mediation Model&lt;/h2&gt;
&lt;p&gt;The key to building a mediation model is to make sure the regressions are in the correct order. In the example above, we need to make sure density is regressed with smoking and family, and family is regressed with smoking. Additionally in Lavaan, we use labels (e.g. a, b, c) to define effects, which we can then manipulate to get the direct (density on smoking) and indirect effects (density on smoking through family smoking).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mediation.model &amp;lt;- 
           &amp;#39; 
           # Measurement model
             family  =~ brother + mother + father + sister
           # Residual correlations
             brother ~~ father
             brother ~~ mother
             sister  ~~ mother
             sister  ~~ father
           # direct effect
             smoking ~ c*density
           # mediator
             family ~  a*density
             smoking ~ b*family
           # indirect effect 
             ab := a*b
           # total effect
             total := c + (a*b)
         &amp;#39;
fit &amp;lt;- sem(mediation.model, data = smokingdata, ordered=&amp;quot;smoking&amp;quot;)
summary(fit, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;modification-indices-model-fit&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modification indices &amp;amp; Model fit&lt;/h2&gt;
&lt;p&gt;The next stage is to check the modification indices to understand whether adding pathways in your model would improve the fit. Crucially, these need to be validated by the literature.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mi &amp;lt;- modindices(fit)
mi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In our example we did not originally include a residual correlation between sister and brother smoking status which probably would have had a high ‘mi’ value and therefore worthy of closer inspection. Once we have a model that we are happy with we can check the model fit. The model fit is calculated by comparing the sample variance/covariance matrix to a derived population sample variance/covariance matrix. There are a number statistics to determine goodness-of-fit (see references at the end of this post).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;moderation-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Moderation Model&lt;/h2&gt;
&lt;p&gt;In the example we want to know if there are significant differences in the association between density and smoking by deprivation. This can be done by running a stratified model, and comparing the coefficients using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Sobel_test&#34;&gt;Sobel test&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;moderation.model &amp;lt;- 
           &amp;#39; 
           # Measurement model
             family  =~ brother + mother + father + sister
           # Residual correlations
             brother ~~ father
             brother ~~ mother
             sister  ~~ mother
             sister  ~~ father
           # direct effect
             smoking ~ c(c1,c2)*density
           # mediator
             family ~  c(a1,a2)*density
             smoking ~ c(b1,b2)*family
          
           # Group 1 - low deprivation
             DirectLOW:= c1
             IndirectLOW:= (a1*b1)
             TotalLOW:= c1 + (a1*b1)
            
           # Group 2 - High deprivation 
             DirectHIGH:= c2
             IndirectHIGH:= (a2*b2)
             TotalHIGH:= c2 + (a2*b2)

           # Sobel test
            DirectDIFF := DirectLOW-DirectHIGH
            IndirectDIFF := IndirectLOW-IndirectHIGH
            TotalDIFF := TotalLOW-TotalHIGH
         &amp;#39;
fit &amp;lt;- sem(moderation.model, data = smokingdata, ordered=&amp;quot;smoking&amp;quot;, group=&amp;quot;deprivation&amp;quot;)
summary(fit, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;standardized-estimates-and-plotting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Standardized estimates and plotting&lt;/h2&gt;
&lt;p&gt;From here, you will want to get standardized estimates for the models using the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;standardizedSolution(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may also be interested to plot the model as a figure using &lt;a href=&#34;http://sachaepskamp.com/semPlot/examples&#34;&gt;‘semPlot’&lt;/a&gt;; an alternative is to construct it yourself using &lt;a href=&#34;https://www.yworks.com/products/yed&#34;&gt;Yed&lt;/a&gt;, with the output that you have generated.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Review literature and sketch out proposed relationships&lt;/li&gt;
&lt;li&gt;Construct latent variables using CFA&lt;/li&gt;
&lt;li&gt;Mediation Model&lt;/li&gt;
&lt;li&gt;Modification Indices &amp;amp; Model fit statistics&lt;/li&gt;
&lt;li&gt;Moderation Model&lt;/li&gt;
&lt;li&gt;Standardized estimates and plotting&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;resources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;What is sem? &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2987867/&#34; class=&#34;uri&#34;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2987867/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Why sem? &lt;a href=&#34;https://academic.oup.com/ije/article/38/2/549/657330&#34; class=&#34;uri&#34;&gt;https://academic.oup.com/ije/article/38/2/549/657330&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Reporting sem: &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.3200/JOER.99.6.323-338&#34; class=&#34;uri&#34;&gt;https://www.tandfonline.com/doi/abs/10.3200/JOER.99.6.323-338&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Model fit guidelines: &lt;a href=&#34;https://www.cscu.cornell.edu/news/Handouts/SEM_fit.pdf&#34; class=&#34;uri&#34;&gt;https://www.cscu.cornell.edu/news/Handouts/SEM_fit.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lavaan syntax: &lt;a href=&#34;http://jeromyanglim.tumblr.com/post/33556941601/lavaan-cheat-sheet&#34; class=&#34;uri&#34;&gt;http://jeromyanglim.tumblr.com/post/33556941601/lavaan-cheat-sheet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How can we understand patterns of prescription use?</title>
      <link>http://www.markcherrie.net/post/sequence-analysis-in-r/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.markcherrie.net/post/sequence-analysis-in-r/</guid>
      <description>&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Longitudinal analysis is important as due to the temporal sequence of exposure then outcome, we can make a stronger case for causality. A derivative of a class of models that fit into the ‘data-mining’ family is sequence analysis. One use of this model is to understand lifetime states, e.g. being employed, being in education, being retired. By understanding these state sequences we can understand how the duration and timing of a state can affect health in the long term. An excellent R package that allows you to conduct sequence analysis is the &lt;a href=&#34;https://cran.r-project.org/web/packages/TraMineR/index.html&#34;&gt;TraMineR&lt;/a&gt; package. Here I’m going to describe how traminer can be used to understand patterns in prescription use.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sequence-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sequence Analysis&lt;/h2&gt;
&lt;p&gt;The first step in sequence analysis is to format the dataset into wide format (time as columns and unit of observation as rows). The function in R to define the sequence is ‘seqdef’. Once we have this object, step 2 is to create visualisations using ‘seqplot’. There are many to choose from, I like the index plot (use argument, ‘type=I’), which shows you all the sequences, so is a raw look at the diversity of sequences (n.b. I recommend using the argument ‘sort=“from.start”’ to get a clearer picture). I also like the distribution plot (‘type=d’); this is a summary of the percentages for each time point that correspond to each state. You can then use the group argument to understand difference by the covariates (refer to your original dataframe with the information on for example, sex, age etc.). There are a number of metrics that can be calculated on the sequence including length, duration of distinct states, transition rates, Shannon entropy and turbulence. These are all descriptive statistics to understand the stability of the sequences over time. Step 3 is to understand sequence dissimilarity, i.e. to calculate how close two sequences are. There are a number of dissimilarity measures (e.g. longest common prefix, optimal matching). The latter is commonly used and is based on how many edits, deletions and substitutions it takes to get from one sequence to another; the result is a ‘cost’ of transformation. Generally there tends to be measures that favour sequencing (e.g. when and what order) or timing (e.g. how long in a state). Step 4 is to use the dissimilarity measure with one of two families for clustering (e.g. hierarchical clustering and partitioning around medoids; using ‘WeightedCluster’ package). The researcher has to specify the number of groups and then assess the cluster quality by looking at several measures (e.g. average silhouette width; -1 to 1; higher is better), and interpret them in relation to the literature. The final stage is to use the cluster groups in a regression analysis as covariates or outcome variables.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Define sequence&lt;/li&gt;
&lt;li&gt;Visualise and descriptive statistics&lt;/li&gt;
&lt;li&gt;Sequence dissimilarity&lt;/li&gt;
&lt;li&gt;Clustering (create clusters, assess quality of clusters, interpret them)&lt;/li&gt;
&lt;li&gt;Regression with cluster groups&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;This &lt;a href=&#34;https://thebest.shinyapps.io/Sequence/&#34;&gt;example&lt;/a&gt; is a simulated dataset on 6-month status of prescription use. If the participant has had a prescription in the last 6 months they are defined as a cases, and subdivided by old/new (whether they had prescriptions in the first 6 months), relapse/norelapse (whether they have come off prescriptions for 6 months and then back on), recurrence/remission (whether on/off prescriptions by the end of the time period); If they have never been prescribed they are classified as “Never”. The app first gives a description of the simulated dataset, the state distribution, state index and state distributions by cluster group.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to find the optimal number of groups in GBTM</title>
      <link>http://www.markcherrie.net/post/group-based-trajectory-modelling-in-r/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.markcherrie.net/post/group-based-trajectory-modelling-in-r/</guid>
      <description>&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Group-based trajectory modelling (GBMT) is a way of identifying latent patterns of change from multiple individual trajectories. It is widely used within the field of economics and also becoming popular in health geography. The process of deciding the number of classes is not transparent and tends to be based on one or two model fit statistics. &lt;a href=&#34;http://journals.sagepub.com/doi/pdf/10.1177/0962280215598665&#34;&gt;Klijn et al&lt;/a&gt;., created a fit-criteria assessment plot (F-CAP) that accepts universal data input to aid the decision on the number of classes (2017).&lt;/p&gt;
&lt;p&gt;As we have several data of the macroeconomy, it is important to visually inspect the model fit statistics and to decide the number of classes before applying in a model with the mental health outcomes. These stages (classify and analyse) are normally done seperately, so the units of analysis are classfied, then class membership is taken as known in the subsequent analysis with the health outcome (i.e. deterministic instead of probabilistic).&lt;/p&gt;
&lt;p&gt;This document describes the process of using F-CAP’s with the example of trajectories of employment (2006-2015) at the Scottish local authority level (n=32).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method&lt;/h2&gt;
&lt;p&gt;We firstly load the employment data that was processed in the macroeconomy document. Some processing is required to get this data ready for the linear class mixed model. The first figure shows a simple visualisation of the employment trajectories for each LA over the time period:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##################### EMPLOYMENT #######################################
# Load packages
library(ggplot2)
library(catspec)
library(gridExtra)
library(lcmm)

# STEP 1: LOAD EMPLOYMENT DATA
data&amp;lt;-readr::read_csv(&amp;quot;Traj_files/LAemployment.csv&amp;quot;)

# STEP 2 FORMAT DATA FOR LCMM
datamanip&amp;lt;-function(Indicator, scale){
  data&amp;lt;-subset(data, IndSub==Indicator)
  
  # Subset for latent class model
  traj &amp;lt;- data
  traj$id &amp;lt;- as.factor(traj$Council)
  traj$x&amp;lt;-traj$Year
  traj$y&amp;lt;-as.numeric(traj$Value)
  traj&amp;lt;-subset(traj, select=c(&amp;quot;id&amp;quot;, &amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;))
  traj$x2&amp;lt;-traj$x^2
  traj$x3&amp;lt;-traj$x^3
  
  # scale by geographic unit!
  library(dplyr)
  if(scale==TRUE){
  traj&amp;lt;-traj %&amp;gt;%
    group_by(id) %&amp;gt;%
    mutate(y=scale(y, center = TRUE, scale = TRUE))
  return(traj)
} else(
  return(traj))
  }
traj&amp;lt;-datamanip(&amp;quot;pct&amp;quot;, &amp;quot;FALSE&amp;quot;)

# viz
ggplot(traj, aes(as.factor(x), y, group=id)) + 
  geom_line(aes(colour=id, group=id)) + 
  labs(x = &amp;quot;Year&amp;quot;, y = &amp;quot;% Employed&amp;quot;, title = &amp;quot;Plot of all LA trajectories&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.markcherrie.net/post/gbtm_files/figure-html/STEP1and2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, we need to fit the LCMM models (this is a batch process taking arguments of ‘formula’, ‘number of groups’ and ‘model’). In this example we are using the linear specification of the employment variables (i.e. we believe that trajectories over time are best modelled by linear terms rather than higher terms), a maximum of 10 latent classes and a linear link (as the employment data is continuous). The model fit statistics are then written into a .csv file that has been formatted to be accepted as universal input by the F-CAP R code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##### STEP 3: FIT THE LCMM MODELS AND OUTPUT MODEL FIT

# Create the file for running the procedure from 2-10 groups
batchit&amp;lt;-function(formula, groups, model){
  batch&amp;lt;-expand.grid(formula, rep(2:groups))
  colnames(batch)&amp;lt;-c(&amp;quot;formula&amp;quot;, &amp;quot;numberofgroups&amp;quot;)
  batch$link&amp;lt;-model
  batch$formula&amp;lt;-as.character(batch$formula)
  batch$numberofgroups&amp;lt;-as.numeric(as.character(batch$numberofgroups))
  return(batch)
}
batch&amp;lt;-batchit(&amp;quot;x&amp;quot;, 10, &amp;quot;linear&amp;quot;)

# Model Fit output function
#file.remove(list.files(&amp;quot;trajectories/fcap/input/&amp;quot;, pattern=&amp;quot;Universal_*&amp;quot;, full.names = T))
modelfittraj&amp;lt;-function(formula, numberofgroups, link){
  d4&amp;lt;-lcmm(as.formula(paste0(&amp;quot;y~&amp;quot;, formula)),
           as.formula(paste0(&amp;quot;mixture=~&amp;quot;, formula)),
           #as.formula(paste0(&amp;quot;random=~&amp;quot;, formula)),
           # whether to include random effects is quite a talking point
           subject=&amp;#39;id&amp;#39;,
           ng=numberofgroups,
           idiag=TRUE, 
           data=traj,
           link=link)
  postprob(d4)
  conv&amp;lt;-ifelse(d4$conv==1, &amp;quot;TRUE&amp;quot;, &amp;quot;FALSE&amp;quot;)
  k&amp;lt;-paste(d4$AIC, d4$BIC, exp(d4$loglik), conv)
  k&amp;lt;-gsub(&amp;quot; &amp;quot;, &amp;quot;,&amp;quot;, k)
  pp&amp;lt;-d4$pprob
  pp&amp;lt;-cbind(pp[,3:(ncol(pp))], d4$pprob$class)
  sink(paste0(&amp;quot;trajectories/fcap/input/&amp;quot;,formula, numberofgroups, link,&amp;quot;.txt&amp;quot;))
  cat(k, &amp;quot;\n&amp;quot;)
  # Stop writing to the file
  sink()
  write.table(pp, paste0(&amp;quot;trajectories/fcap/input/&amp;quot;,formula, numberofgroups, link,&amp;quot;.txt&amp;quot;), 
              append=TRUE, 
              sep = &amp;quot;,&amp;quot;,
              row.names = F,
              col.names = F)
  output&amp;lt;-read.table(paste0(&amp;quot;trajectories/fcap/input/&amp;quot;,formula, numberofgroups, link,&amp;quot;.txt&amp;quot;), row.names=NULL, sep=&amp;quot;,&amp;quot;, fill=T, header=F)
  file.remove(paste0(&amp;quot;trajectories/fcap/input/&amp;quot;,formula, numberofgroups, link,&amp;quot;.txt&amp;quot;))
  output[is.na(output)]&amp;lt;-&amp;quot;&amp;quot;
  readr::write_csv(output, paste0(&amp;quot;trajectories/fcap/input/&amp;quot;,&amp;quot;Universal_&amp;quot;, numberofgroups,&amp;quot;.csv&amp;quot;), col_names = F)
}
# Batch it
library(plyr)
#mdply(batch,modelfittraj)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then take the model fit output and use it as input to the F-CAP function that was supplied by Valeria Lima Passos (co-author of the paper above).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##### STEP 4 - Run the CSV files through F-CAP

#source(paste0(&amp;quot;trajectories/fcap/fcap_base_12_PC.R&amp;quot;), echo=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next step is to look at the F-CAPs and make a decision on the number of classes. Note, where there are + or - signs after some of the model fit statistics, this is the highest and lowest per group, as opposed to the mean. The first plot shows the AIC and BIC (penalises model complexity to a higher degree than AIC). Generally for these two, the lower the number the better the model fit. Six classes has the lowest BIC, but there could be an argument for 4 clases being the elbow.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;img/Traj_images/AICBIC.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The average posterior probability of assignment (APPA) shows the probability of belonging to a certain class. Generally, over 70% for APPA is defined as sufficient. The SD is standard deviation of group membership probabilities and is generally lower when model fit is better. The mismatch between estimated and assigned group probabilities (Mismatch) in a perfect model is zero. In this case, all of the classes have over 70%, with the highest probabilities for 2, then 4 and 5 classes. The mismatch is close to zero for all classes, with slight increasing trend with increasing number of classes. The SD is lowest for 2 groups and is noticebly higher after 5 classes.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;img/Traj_images/PP.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The Odds of Correct Classification (OCC) compares the average posterior probability assignment, correcting for OCC based on random assignment. Generally, higher OCC’s indicate better model fit with a threshold of 5 used to show good assignment accuracy. In this case, all classes have sufficient accuracy in class assignment but especially so for classes 3 to 9.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;img/Traj_images/OCC.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;We tend not to want classes that contain a very small number of units. A common threshold to apply is that each group will have at least 1% of the sample. However this should be modified if total number of units in the sample are low. In this case, although we are above the 1% threshold in all models with under 6 classes, because we have so few in the sample (N=32), a suitable number of classes would 2,3 or 4, given that at least ~10% of the sample are in each class.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;img/Traj_images/Small.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;In conclusion, the best number of classes in the model of LA employment trajectories from 2006-2015 is four. Although some of the criteria showed little variation between the classes (OCC, Mismatch), we arrived at as this number of classes using the following criteria:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BIC elbow&lt;/li&gt;
&lt;li&gt;High posterior probabilities (&amp;gt;90%)&lt;/li&gt;
&lt;li&gt;Relatively low standard deviation of PP within each class&lt;/li&gt;
&lt;li&gt;Had at least ~10% of the sample in any one class&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The next step is to then see whether changing the polynomial order gives any improvement in these model fit statistics (although in R I can only seem to change all the clusters at once rather than individually). For brevity, this is not shown in this document.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## What&amp;#39;s the best model?

##### STEP 5 - Finalise the group membership and plot

# plot traj
plottraj&amp;lt;-function(formula, numberofgroups, link, output){
  k&amp;lt;-NULL
  traj$id &amp;lt;- as.factor(traj$id)
  d4&amp;lt;-lcmm(as.formula(paste0(&amp;quot;y~&amp;quot;, formula)),
           as.formula(paste0(&amp;quot;mixture=~&amp;quot;, formula)),
           subject=&amp;#39;id&amp;#39;,
           ng=numberofgroups,
           idiag=TRUE, 
           data=traj,
           link=link)
  postprob(d4)
  traj$id2 &amp;lt;- as.numeric(traj$id)
  groups &amp;lt;- as.data.frame(d4$pprob[,1:2])
  traj$TRAJGROUP &amp;lt;- factor(groups$class[sapply(traj$id2, function(x) which(groups$id==x))])
if (output==&amp;quot;plot&amp;quot;){
ggplot(traj, aes(x, y, group=id, colour=group2)) + geom_line() + geom_smooth(aes(group=group2), method=&amp;quot;loess&amp;quot;, size=2, se=F) + labs(x=&amp;quot;x&amp;quot;,y=&amp;quot;y&amp;quot;,colour=&amp;quot;Latent Class&amp;quot;)
  pl&amp;lt;-ggplot(traj, aes(as.factor(x), y, group=id, colour=TRAJGROUP)) + geom_smooth(aes(group=id, colour=TRAJGROUP),size=0.5, se=F) + geom_smooth(aes(group=TRAJGROUP), method=&amp;quot;loess&amp;quot;, size=2, se=T) + labs(x=&amp;quot;Date&amp;quot;,y=&amp;quot;% Employed&amp;quot;,colour=&amp;quot;Latent Class&amp;quot;)
#  ggsave(paste0(&amp;quot;plots/&amp;quot;,formula, #numberofgroups, link,&amp;quot;.png&amp;quot;), plot=pl)
  pl

} else if (output==&amp;quot;file&amp;quot;){write.csv(traj, paste0(&amp;quot;data/&amp;quot;,formula, numberofgroups, link,&amp;quot;.csv&amp;quot;),row.names=F)}
}

# Put in the F-CAP best model here
plottraj(&amp;quot;x&amp;quot;, 4, &amp;quot;linear&amp;quot;, &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Be patient, lcmm is running ... 
## The program took 1.2 seconds 
##  
## Posterior classification: 
##   class1 class2 class3 class4
## N   3.00  15.00   3.00  11.00
## %   9.38  46.88   9.38  34.38
##  
## Posterior classification table: 
##      --&amp;gt; mean of posterior probabilities in each class 
##         prob1  prob2  prob3  prob4
## class1 0.9993 0.0007 0.0000 0.0000
## class2 0.0000 0.9976 0.0000 0.0024
## class3 0.0000 0.0000 0.9998 0.0002
## class4 0.0000 0.0476 0.0005 0.9519
##  
## Posterior probabilities above a threshold (%): 
##          class1 class2 class3 class4
## prob&amp;gt;0.7    100    100    100  90.91
## prob&amp;gt;0.8    100    100    100  90.91
## prob&amp;gt;0.9    100    100    100  90.91
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.markcherrie.net/post/gbtm_files/figure-html/STEP5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The groups are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Group 1: Orkney Islands, Shetland Islands, Aberdeenshire&lt;/li&gt;
&lt;li&gt;Group 2: East Lothian, East Renfrewshire, Eilean Siar, Falkirk, Fife, Highland, Midlothian, Moray, Perth and Kinross, Scottish Borders, Aberdeen City, Argyll and Bute , West Lothian, Angus, East Dunbartonshire&lt;/li&gt;
&lt;li&gt;Group 3: North Ayrshire, Dundee City, Glasgow City&lt;/li&gt;
&lt;li&gt;Group 4: Clackmannanshire, Dumfries and Galloway, East Ayrshire, Inverclyde, South Ayrshire, South Lanarkshire, Stirling, City of Edinburgh, Renfrewshire, West Dunbartonshire, North Lanarkshire&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;n.b We do not include random effects in the above models. These are not included in the the SAS and Stata procedures for GBTM. The implication is that we do not allow variation within or between classes. The practical implication is that this will result in a greater number of latent classes being selected (Further discussion in &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3085403/&#34;&gt;Saunders&lt;/a&gt;, 2011).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
