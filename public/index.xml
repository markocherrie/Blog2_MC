<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mark Cherrie on Mark Cherrie</title>
    <link>/</link>
    <description>Recent content in Mark Cherrie on Mark Cherrie</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019 Mark Cherrie</copyright>
    <lastBuildDate>Tue, 29 Aug 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Using Social Media to Engage Research End Users: Thoughts from UoE Research Support Office Event</title>
      <link>/post/socialmedia/</link>
      <pubDate>Wed, 08 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/socialmedia/</guid>
      <description>


&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Social media is becoming the dominant platform for getting information, for the general population by superseding television and newspapers and increasingly for academics, displacing peer-reviewed journal articles. Time is limited and short communications are easier to digest, especially when on a mobile.&lt;/p&gt;
&lt;p&gt;To summarise findings from a research project or engage in an academic/policy debate in very short social media snippets is an impressive skill and one that I am keen to learn. This was the motivation behind going on the Research Support Office event on &lt;a href=&#34;https://www.ed.ac.uk/arts-humanities-soc-sci/research-ke/support-for-staff/staff-development/learning-lunches-events/using-social-media-to-engage-research-end-users&#34;&gt;‘Using social media to engage research end users’&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-use-social-media&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why use social media?&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Dissemination - share cutting-edge research&lt;/li&gt;
&lt;li&gt;Listening - retweet/like other research (a personalised newsfeed)&lt;/li&gt;
&lt;li&gt;Connecting - reuse content and feedback to contributors, get advice on research from peers, conference hashtag (if it’s your event make sure hashtag isn’t already taken!)&lt;/li&gt;
&lt;li&gt;Interacting - plan ahead, make it regular, half an hour a day?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;They recommended that to start it might be useful to have a go with two types of social media platforms:&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;types-of-social-media&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Types of social media&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Project blog - Keep it short (&amp;lt; 500 words) and informal, UoE has an &lt;a href=&#34;https://www.ed.ac.uk/information-services/learning-technology/blogging&#34;&gt;in-house service for Wordpress blogging&lt;/a&gt;, which comes with lots of support (e.g. security, GDPR compliancy)&lt;/li&gt;
&lt;li&gt;Twitter - always include imagery (and make it fun)&lt;/li&gt;
&lt;li&gt;Facebook - good for events&lt;/li&gt;
&lt;li&gt;E-newsletter&lt;/li&gt;
&lt;li&gt;Infographics&lt;/li&gt;
&lt;li&gt;Video/Audio channels&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There is a number of free open source software to help with getting started:&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;software&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Software&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://tweetdeck.twitter.com/&#34;&gt;Tweetdeck&lt;/a&gt; - schedule tweets for times of day (some times/days more effective - most effective times are subject specific)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wakelet.com/findoutmore.html&#34;&gt;Wakelet&lt;/a&gt; - gather social media data to tell a story (for an event for example)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.canva.com/&#34;&gt;Canva&lt;/a&gt; - online graphic design for images, infographics etc.&lt;/li&gt;
&lt;li&gt;Google/twitter analytics - measuring engagement&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://unsplash.com/&#34;&gt;Unsplash&lt;/a&gt; - free high resolution images&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Geography of the Revoke Article 50 petition</title>
      <link>/post/geography_of_the_revoke_article_50_petition/</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/geography_of_the_revoke_article_50_petition/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The ‘Revoke article 50 and remain in the EU petition’ is the most popular petition ever, which aims to stop the brexit process. The rules are that after 10,000 signatures, petitions get a response from the government, after 100,000 signatures, petitions are considered for debate in Parliament and after 17.4 million, we stop Brexit (Andrea Leadsom, 2019). As of writing this (25/03/19 17:00 GMT), the petition has amassed over 5 million signatures. As a geographer, I wondered whether there were some places that were signing (read that as sighing on a proofread… probably accurate) a lot more than others and whether that was expected, given how their constituency voted in the referendum.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method&lt;/h2&gt;
&lt;p&gt;Luckily, the required data at the constituency level is publically available for: the number of signatures of the &lt;a href=&#34;https://petition.parliament.uk/petitions/241584?fbclid=IwAR2E3C5chLiMm9MM7awL2qWw_Fkr_KLXert0S61swiRbYORAfmOOglgZ8as&#34;&gt;‘Revoke article 50’ petition&lt;/a&gt;, boundaries of the constituencies with the data on the &lt;a href=&#34;http://www.statsmapsnpix.com/2017/04/getting-ready-for-ge2017-big-shapefile.html&#34;&gt;2015 electorate&lt;/a&gt; and the &lt;a href=&#34;https://commonslibrary.parliament.uk/parliament-and-elections/elections-elections/brexit-votes-by-constituency/&#34;&gt;% voting to leave in the referendum&lt;/a&gt;. After merging these data together I first calculated the percentage of electorate who signed the petition. I then built a regression model using the electorate petition percentage as the dependent variable and the Brexit leave estimate as the independent variable.&lt;/p&gt;
&lt;p&gt;The hypothesis is that the areas with higher percentages of leave voters would have the lowest percentage of the electorate signing the petition. By studying the residuals we can see constituencies that are signing the petition more or less than we would expect (given the referendum vote). I visualised the data as a &lt;a href=&#34;http://www.dannydorling.org/?page_id=3132&#34;&gt;cartogram&lt;/a&gt;, distorting the map to enlarge constituencies that had a) higher petition percentages b) higher-than-expected petition percentages.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;The highest percentages of petition signatures were: Hornsey and Wood Green (38%), Bristol West (37%), Cities of London and Westminster (37%), Brighton, Pavilion (35%) and Islington North (34%); and the lowest: Barnsley East (3%), Birmingham, Hodge Hill (3%), Doncaster North (3%), Dudley North (3%) and Easington (3%). Here’s the map of percentage of constituency signing the petition:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;img/brexit_images/percentageofelectorate.png&#34; alt=&#34;Where have people been signing the petition?&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;em&gt;Where have people been signing the petition?&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The Brexit referendum vote explained 69% of the variance of the petition signature percentage. For every percentage increase in the Brexit leave vote there was a -0.46% lower petition percentage (95% CI -0.48 to -0.44). There was a general North West/South East pattern in the model residuals, with much lower than expected petition percentages coming from Northern Ireland and much higher than expected petition percentages coming from London and Brighton. Here’s the cartogram on expected petition percentage given the referendum’s vote:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;img/brexit_images/revokearticlebrexitref.png&#34; alt=&#34;Where has the petition had higher signatures than expected?&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;em&gt;Where has the petition had higher signatures than expected?&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://petition.parliament.uk/petitions/241584?fbclid=IwAR2E3C5chLiMm9MM7awL2qWw_Fkr_KLXert0S61swiRbYORAfmOOglgZ8as&#34;&gt;Sign up&lt;/a&gt;, especially if you are in Northern England, Wales, Scotland and Northern Ireland and we might hit 17.4 million ☺&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Quantifying exposure to the natural environment across the lifecourse: Implications for health inequalities</title>
      <link>/talk/openspace2019/</link>
      <pubDate>Mon, 18 Mar 2019 12:00:00 +0000</pubDate>
      
      <guid>/talk/openspace2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How can we measure everyday exposure to greenspaces?</title>
      <link>/post/everydaygreen/</link>
      <pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/everydaygreen/</guid>
      <description>


&lt;p&gt;How can we estimate exposure to the natural environment day-to-day?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Geospatial Research Methods</title>
      <link>/talk/geospatialresearchmethods-2019/</link>
      <pubDate>Thu, 07 Mar 2019 12:00:00 +0000</pubDate>
      
      <guid>/talk/geospatialresearchmethods-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Association between activity space exposure to parks in childhood and adolescence and cognitive ageing in later life</title>
      <link>/publication/activityspace/</link>
      <pubDate>Mon, 18 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/activityspace/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tobacco Outlet Density and Tobacco/smoking knowledge in Scottish adolescents</title>
      <link>/talk/stadisplay/</link>
      <pubDate>Thu, 17 Jan 2019 11:30:00 +0000</pubDate>
      
      <guid>/talk/stadisplay/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How to build a structural equation model in Lavaan</title>
      <link>/post/structural-equation-modelling-in-r/</link>
      <pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/structural-equation-modelling-in-r/</guid>
      <description>


&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;I went on a &lt;a href=&#34;https://www.psychometrics.cam.ac.uk/trainingworkshops/structural-equation-modelling-in-r-4-day-course&#34;&gt;course&lt;/a&gt; in Cambridge over the summer of 2018. This was to get me up to speed on structural equation modelling (SEM), which has a lot of potential applications in scenarios where the pathways between measured and unmeasured variables are the central focus of the research question.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-sem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is SEM?&lt;/h2&gt;
&lt;p&gt;SEM is a mixture of confirmatory factor analysis (CFA) and path analysis. Another way to describe that, is that you have a measurement part and a structural part. They relate to two questions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How can we measure latent concepts?&lt;/li&gt;
&lt;li&gt;What is the relationship between the variables in our model?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The key point to structural equation modelling is that is splits measurement error and the correlation between the residuals from the structural coefficients. I recommend the Lavaan &lt;a href=&#34;http://lavaan.ugent.be/tutorial/est.html&#34;&gt;tutorial&lt;/a&gt;; below is my workflow for building a mediation/moderation model in Lavaan.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lavaan-syntax&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lavaan syntax&lt;/h2&gt;
&lt;p&gt;First of all the syntax for Lavaan models is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;~ Define regression formula&lt;/li&gt;
&lt;li&gt;~~ Define correlated residual variances (two observed variables)&lt;/li&gt;
&lt;li&gt;~= Define latent variable&lt;/li&gt;
&lt;li&gt;:= Define effect (i.e. indirect or total)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Remember, you’ll need to define the model in speech marks and then use it as the model argument in the lavaan functions: cfa and sem.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;This example is on understanding the pathways to smoking in adolescents. The density of tobacco retailers in an adolescents’ residential neighbourhood predicts propensity to smoke; the greater the density of retailers the more likely smoking is &lt;a href=&#34;https://tobaccocontrol.bmj.com/content/25/1/75.info&#34;&gt;(Shortt et al., 2014)&lt;/a&gt;. However we know that density also predicts whether a family member will smoke and that if a family member is a smoker, the adolescent is more likely to smoke. Also, we know that the relationship between density of retailer and smoking is stronger when the individual has a lower socioeconomic status or lives in an area of higher deprivation &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/15965140&#34;&gt;(Chuang et al., 2005)&lt;/a&gt;. How can we model these latent contructs (family smoking) and complex relationships? This hypothetical dataset called ‘smokingdata’ has the following variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;density: continuous density of tobacco retailers&lt;/li&gt;
&lt;li&gt;smoking: binary smoking status&lt;/li&gt;
&lt;li&gt;family: family smoking latent variable&lt;/li&gt;
&lt;li&gt;brother: measured variable on smoking status in brother&lt;/li&gt;
&lt;li&gt;mother: measured variable on smoking status in mother&lt;/li&gt;
&lt;li&gt;sister: measured variable on smoking status in sister&lt;/li&gt;
&lt;li&gt;father: measured variable on smoking status in father&lt;/li&gt;
&lt;li&gt;deprivation: binary variable of deprivation in local neighbourhood&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;construct-latent-variables-using-cfa&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Construct latent variables using CFA&lt;/h2&gt;
&lt;p&gt;The first stage is to identify the latent constructs and the variables, which will be used to predict it (3 variables is minimum). Compare the model fit (use ‘fit.measures=T’) of several models and with what you expected given your reading of the literature.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lavaan)
cfa.model &amp;lt;- 
           &amp;#39; 
           # Measurement model
             family  =~ brother + mother + father + sister
           &amp;#39;
fit &amp;lt;- cfa(cfa.model, data = smokingdata, ordered=c(&amp;quot;brother&amp;quot;,&amp;quot;mother&amp;quot;,&amp;quot;father&amp;quot;,&amp;quot;sister&amp;quot;))
summary(fit, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mediation-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mediation Model&lt;/h2&gt;
&lt;p&gt;The key to building a mediation model is to make sure the regressions are in the correct order. In the example above, we need to make sure density is regressed with smoking and family, and family is regressed with smoking. Additionally in Lavaan, we use labels (e.g. a, b, c) to define effects, which we can then manipulate to get the direct (density on smoking) and indirect effects (density on smoking through family smoking).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mediation.model &amp;lt;- 
           &amp;#39; 
           # Measurement model
             family  =~ brother + mother + father + sister
           # Residual correlations
             brother ~~ father
             brother ~~ mother
             sister  ~~ mother
             sister  ~~ father
           # direct effect
             smoking ~ c*density
           # mediator
             family ~  a*density
             smoking ~ b*family
           # indirect effect 
             ab := a*b
           # total effect
             total := c + (a*b)
         &amp;#39;
fit &amp;lt;- sem(mediation.model, data = smokingdata, ordered=&amp;quot;smoking&amp;quot;)
summary(fit, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;modification-indices-model-fit&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modification indices &amp;amp; Model fit&lt;/h2&gt;
&lt;p&gt;The next stage is to check the modification indices to understand whether adding pathways in your model would improve the fit. Crucially, these need to be validated by the literature.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mi &amp;lt;- modindices(fit)
mi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In our example we did not originally include a residual correlation between sister and brother smoking status which probably would have had a high ‘mi’ value and therefore worthy of closer inspection. Once we have a model that we are happy with we can check the model fit. The model fit is calculated by comparing the sample variance/covariance matrix to a derived population sample variance/covariance matrix. There are a number statistics to determine goodness-of-fit (see references at the end of this post).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;moderation-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Moderation Model&lt;/h2&gt;
&lt;p&gt;In the example we want to know if there are significant differences in the association between density and smoking by deprivation. This can be done by running a stratified model, and comparing the coefficients using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Sobel_test&#34;&gt;Sobel test&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;moderation.model &amp;lt;- 
           &amp;#39; 
           # Measurement model
             family  =~ brother + mother + father + sister
           # Residual correlations
             brother ~~ father
             brother ~~ mother
             sister  ~~ mother
             sister  ~~ father
           # direct effect
             smoking ~ c(c1,c2)*density
           # mediator
             family ~  c(a1,a2)*density
             smoking ~ c(b1,b2)*family
          
           # Group 1 - low deprivation
             DirectLOW:= c1
             IndirectLOW:= (a1*b1)
             TotalLOW:= c1 + (a1*b1)
            
           # Group 2 - High deprivation 
             DirectHIGH:= c2
             IndirectHIGH:= (a2*b2)
             TotalHIGH:= c2 + (a2*b2)

           # Sobel test
            DirectDIFF := DirectLOW-DirectHIGH
            IndirectDIFF := IndirectLOW-IndirectHIGH
            TotalDIFF := TotalLOW-TotalHIGH
         &amp;#39;
fit &amp;lt;- sem(moderation.model, data = smokingdata, ordered=&amp;quot;smoking&amp;quot;, group=&amp;quot;deprivation&amp;quot;)
summary(fit, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;standardized-estimates-and-plotting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Standardized estimates and plotting&lt;/h2&gt;
&lt;p&gt;From here, you will want to get standardized estimates for the models using the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;standardizedSolution(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may also be interested to plot the model as a figure using &lt;a href=&#34;http://sachaepskamp.com/semPlot/examples&#34;&gt;‘semPlot’&lt;/a&gt;; an alternative is to construct it yourself using &lt;a href=&#34;https://www.yworks.com/products/yed&#34;&gt;Yed&lt;/a&gt;, with the output that you have generated.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Review literature and sketch out proposed relationships&lt;/li&gt;
&lt;li&gt;Construct latent variables using CFA&lt;/li&gt;
&lt;li&gt;Mediation Model&lt;/li&gt;
&lt;li&gt;Modification Indices &amp;amp; Model fit statistics&lt;/li&gt;
&lt;li&gt;Moderation Model&lt;/li&gt;
&lt;li&gt;Standardized estimates and plotting&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;resources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;What is sem? &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2987867/&#34; class=&#34;uri&#34;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2987867/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Why sem? &lt;a href=&#34;https://academic.oup.com/ije/article/38/2/549/657330&#34; class=&#34;uri&#34;&gt;https://academic.oup.com/ije/article/38/2/549/657330&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Reporting sem: &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.3200/JOER.99.6.323-338&#34; class=&#34;uri&#34;&gt;https://www.tandfonline.com/doi/abs/10.3200/JOER.99.6.323-338&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Model fit guidelines: &lt;a href=&#34;https://www.cscu.cornell.edu/news/Handouts/SEM_fit.pdf&#34; class=&#34;uri&#34;&gt;https://www.cscu.cornell.edu/news/Handouts/SEM_fit.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lavaan syntax: &lt;a href=&#34;http://jeromyanglim.tumblr.com/post/33556941601/lavaan-cheat-sheet&#34; class=&#34;uri&#34;&gt;http://jeromyanglim.tumblr.com/post/33556941601/lavaan-cheat-sheet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to quickly visualise massive spatio-temporal data</title>
      <link>/post/uber-kepler/</link>
      <pubDate>Sat, 22 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/uber-kepler/</guid>
      <description>


&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Are you collecting lots of geospatial data and need a way to quickly visualise what you’ve got? Read on! I had this problem; I was collecting a lot of information on my daily movements from &lt;a href=&#34;https://itunes.apple.com/us/app/arc-app-location-activity/id1063151918?mt=8&#34;&gt;Arc App: Location and Activity&lt;/a&gt; (more on the use of Arc for research later). Additionally, I wanted to quickly see what features of the urban environment (e.g. greenspaces) I was being ‘exposed’ to. The solution was Uber’s open source geospatial analysis tool: &lt;a href=&#34;https://kepler.gl&#34;&gt;Kepler.gl&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method&lt;/h2&gt;
&lt;p&gt;Some formatting of your data is required, but it’s pretty simple, just output a CSV with a &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/base/html/as.POSIXlt.html&#34;&gt;datetime&lt;/a&gt;, latitude and longitude column, alongside any other attribute data that you are interested in (here I’m interested in greenspace exposure, i.e. when I was within 50 metres of a &lt;a href=&#34;https://www.ordnancesurvey.co.uk/business-and-government/products/os-open-greenspace.html&#34;&gt;greenspace&lt;/a&gt;). That’s the hard bit. After this, you have lots of options for styling your visualisation. You receive an automatic animation of your data over time, which you have control of with a set of buttons to determine playback. A notable, but more time-consuming, alternative to creating animations from data is the &lt;a href=&#34;https://medium.com/@tjukanov/geogiffery-in-a-nutshell-introduction-to-qgis-time-manager-31bb79f2af19&#34;&gt;QGIS Time manager&lt;/a&gt;.&lt;br /&gt;
Once you like the look of your visualisation, there are options to share as an image or a URL. If, like me, you want to do furthering editing (e.g. annotate certain frames) then you can do a screen recording and then import to a video editing program such as iMovie (n.b. There is probably a better way of doing this). Anyway, hopefully this blog will give enough information to get started with kepler.gl. I was able to create a visualisation of my contact with greenspaces over a 6 months period, which is shown below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualisation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualisation&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/5K49kbiK-Q0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hard times: mental health under austerity</title>
      <link>/talk/foss/</link>
      <pubDate>Wed, 07 Nov 2018 11:30:00 +0000</pubDate>
      
      <guid>/talk/foss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How to make teaching/outreach more interactive</title>
      <link>/post/mentimeter/</link>
      <pubDate>Sat, 03 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/mentimeter/</guid>
      <description>


&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;I am keen to employ technology to assist in teaching. I went to the &lt;a href=&#34;https://socsocmed.org.uk/annual-scientific-meeting/&#34;&gt;Social Science and Medicine Annual Scientific meeting&lt;/a&gt; in Glasgow last September and noticed that they were using software to get responses from the audience. The software was called &lt;a href=&#34;https://www.mentimeter.com/&#34;&gt;Mentimeter&lt;/a&gt;. It turns out there has been a &lt;a href=&#34;http://evs.psy.gla.ac.uk/papers/draperbrown.pdf&#34;&gt;paper&lt;/a&gt; published by researchers from the University of Glasgow on the merits of interactive polling for teaching. The authors summarise that experiences of this technology are generally positive but caution that future use should focus first on the pedagogical approach. Two approaches are put forward: 1) launching student discussion, and 2) to focus on developing teaching aligned to the students questions and needs. I’ll now give a brief description on how to carry out a Mentimeter polling session and how I have used it in teaching and outreach.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mentimeter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mentimeter&lt;/h2&gt;
&lt;p&gt;Mentimeter is proprietary cloud-based software that can collect, record and visualise participant responses via interaction with a webpage. The teacher first asks the students to go onto the webpage and input a code to get to the right question (questions are created on the Mentimeter site, which is very intuitive). The teacher then activates the voting and results are visualised in real-time on the screen. I’ve found the easiest way to include polls in a presentation is to use the Powerpoint &lt;a href=&#34;https://www.mentimeter.com/powerpoint&#34;&gt;plugin&lt;/a&gt;. The plots and data can then be downloaded and are useful for reports after the event.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;examples-of-use&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Examples of use&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Teaching: Honing in on plans for dissertations by asking what type of data students anticipate collecting, to stimulate discussion on the merits of these data.&lt;/li&gt;
&lt;li&gt;Outreach: Developing a structural equation model by asking experts to categorise pathways by their strength of association with outcomes&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;thoughts-on-use&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Thoughts on use&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The use of an internet-enabled device can be exclusionary, if possible there should be devices that can be used by students that are ready to vote.&lt;/li&gt;
&lt;li&gt;In addition to the previous point, it might be beneficial to mix more traditional interactive methods (e.g. the Carousel Activity) with the new technology so there’s a greater chance that people feel comfortable with the type of interaction.&lt;/li&gt;
&lt;li&gt;The data and plots generated from interactive polling are so useful for reflection and it is much less time-consuming than transcribing hand written notes.&lt;/li&gt;
&lt;li&gt;There may be limitations to the type of questions that can be asked and the type of answers that can be supplied (e.g. doodles). In relation to the first point, especially in small groups with questions that might be more personal (i.e. related to their project) some students might not feel confident having their answers shared among the group, and this should be respected.&lt;/li&gt;
&lt;li&gt;Although I have found the technology to be robust, there are always going to be challenges with connectivity. The polling should be a pivot point not the key part of the teaching.&lt;/li&gt;
&lt;li&gt;I would think that spending more time on getting everyone up to speed with the concept of polling and having fewer key questions is better than trying to fit in lots of questions (also relates to student &lt;a href=&#34;https://www.google.com/search?q=response+fatigue&amp;amp;oq=response+fatigue&amp;amp;aqs=chrome..69i57j0l5.2592j0j4&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&#34;&gt;respondent fatigue&lt;/a&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What are your experiences with interactive polling for teaching/outreach?&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How can we understand patterns of prescription use?</title>
      <link>/post/sequence-analysis-in-r/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/sequence-analysis-in-r/</guid>
      <description>


&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Longitudinal analysis is important as due to the temporal sequence of exposure then outcome, we can make a stronger case for causality. A derivative of a class of models that fit into the ‘data-mining’ family is sequence analysis. One use of this model is to understand lifetime states, e.g. being employed, being in education, being retired. By understanding these state sequences we can understand how the duration and timing of a state can affect health in the long term. An excellent R package that allows you to conduct sequence analysis is the &lt;a href=&#34;https://cran.r-project.org/web/packages/TraMineR/index.html&#34;&gt;TraMineR&lt;/a&gt; package. Here I’m going to describe how traminer can be used to understand patterns in prescription use.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sequence-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sequence Analysis&lt;/h2&gt;
&lt;p&gt;The first step in sequence analysis is to format the dataset into wide format (time as columns and unit of observation as rows). The function in R to define the sequence is ‘seqdef’. Once we have this object, step 2 is to create visualisations using ‘seqplot’. There are many to choose from, I like the index plot (use argument, ‘type=I’), which shows you all the sequences, so is a raw look at the diversity of sequences (n.b. I recommend using the argument ‘sort=“from.start”’ to get a clearer picture). I also like the distribution plot (‘type=d’); this is a summary of the percentages for each time point that correspond to each state. You can then use the group argument to understand difference by the covariates (refer to your original dataframe with the information on for example, sex, age etc.). There are a number of metrics that can be calculated on the sequence including length, duration of distinct states, transition rates, Shannon entropy and turbulence. These are all descriptive statistics to understand the stability of the sequences over time. Step 3 is to understand sequence dissimilarity, i.e. to calculate how close two sequences are. There are a number of dissimilarity measures (e.g. longest common prefix, optimal matching). The latter is commonly used and is based on how many edits, deletions and substitutions it takes to get from one sequence to another; the result is a ‘cost’ of transformation. Generally there tends to be measures that favour sequencing (e.g. when and what order) or timing (e.g. how long in a state). Step 4 is to use the dissimilarity measure with one of two families for clustering (e.g. hierarchical clustering and partitioning around medoids; using ‘WeightedCluster’ package). The researcher has to specify the number of groups and then assess the cluster quality by looking at several measures (e.g. average silhouette width; -1 to 1; higher is better), and interpret them in relation to the literature. The final stage is to use the cluster groups in a regression analysis as covariates or outcome variables.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Define sequence&lt;/li&gt;
&lt;li&gt;Visualise and descriptive statistics&lt;/li&gt;
&lt;li&gt;Sequence dissimilarity&lt;/li&gt;
&lt;li&gt;Clustering (create clusters, assess quality of clusters, interpret them)&lt;/li&gt;
&lt;li&gt;Regression with cluster groups&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;This example is a simulated dataset on 6-month status of prescription use. If the participant has had a prescription in the last 6 months they are defined as a cases, and subdivided by old/new (whether they had prescriptions in the first 6 months), relapse/norelapse (whether they have come off prescriptions for 6 months and then back on), recurrence/remission (whether on/off prescriptions by the end of the time period); If they have never been prescribed they are classified as “Never”. The app first gives a description of the simulated dataset, the state distribution, state index and state distributions by cluster group.&lt;/p&gt;
&lt;iframe src=&#34;https://thebest.shinyapps.io/Sequence/?showcase=0&#34; width=&#34;672&#34; height=&#34;400px&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to find the optimal number of groups in GBTM</title>
      <link>/post/group-based-trajectory-modelling-in-r/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/group-based-trajectory-modelling-in-r/</guid>
      <description>


&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Group-based trajectory modelling (GBMT) is a way of identifying latent patterns of change from multiple individual trajectories. It is widely used within the field of economics and also becoming popular in health geography. The process of deciding the number of classes is not transparent and tends to be based on one or two model fit statistics. &lt;a href=&#34;http://journals.sagepub.com/doi/pdf/10.1177/0962280215598665&#34;&gt;Klijn et al&lt;/a&gt;., created a fit-criteria assessment plot (F-CAP) that accepts universal data input to aid the decision on the number of classes (2017).&lt;/p&gt;
&lt;p&gt;As we have several data of the macroeconomy, it is important to visually inspect the model fit statistics and to decide the number of classes before applying in a model with the mental health outcomes. These stages (classify and analyse) are normally done seperately, so the units of analysis are classfied, then class membership is taken as known in the subsequent analysis with the health outcome (i.e. deterministic instead of probabilistic).&lt;/p&gt;
&lt;p&gt;This document describes the process of using F-CAP’s with the example of trajectories of employment (2006-2015) at the Scottish local authority level (n=32).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method&lt;/h2&gt;
&lt;p&gt;We firstly load the employment data that was processed in the macroeconomy document. Some processing is required to get this data ready for the linear class mixed model. The first figure shows a simple visualisation of the employment trajectories for each LA over the time period:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##################### EMPLOYMENT #######################################
# Load packages
library(ggplot2)
library(catspec)
library(gridExtra)
library(lcmm)

# STEP 1: LOAD EMPLOYMENT DATA
data&amp;lt;-readr::read_csv(&amp;quot;Traj_files/LAemployment.csv&amp;quot;)

# STEP 2 FORMAT DATA FOR LCMM
datamanip&amp;lt;-function(Indicator, scale){
  data&amp;lt;-subset(data, IndSub==Indicator)
  
  # Subset for latent class model
  traj &amp;lt;- data
  traj$id &amp;lt;- as.factor(traj$Council)
  traj$x&amp;lt;-traj$Year
  traj$y&amp;lt;-as.numeric(traj$Value)
  traj&amp;lt;-subset(traj, select=c(&amp;quot;id&amp;quot;, &amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;))
  traj$x2&amp;lt;-traj$x^2
  traj$x3&amp;lt;-traj$x^3
  
  # scale by geographic unit!
  library(dplyr)
  if(scale==TRUE){
  traj&amp;lt;-traj %&amp;gt;%
    group_by(id) %&amp;gt;%
    mutate(y=scale(y, center = TRUE, scale = TRUE))
  return(traj)
} else(
  return(traj))
  }
traj&amp;lt;-datamanip(&amp;quot;pct&amp;quot;, &amp;quot;FALSE&amp;quot;)

# viz
ggplot(traj, aes(as.factor(x), y, group=id)) + 
  geom_line(aes(colour=id, group=id)) + 
  labs(x = &amp;quot;Year&amp;quot;, y = &amp;quot;% Employed&amp;quot;, title = &amp;quot;Plot of all LA trajectories&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/gbtm_files/figure-html/STEP1and2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, we need to fit the LCMM models (this is a batch process taking arguments of ‘formula’, ‘number of groups’ and ‘model’). In this example we are using the linear specification of the employment variables (i.e. we believe that trajectories over time are best modelled by linear terms rather than higher terms), a maximum of 10 latent classes and a linear link (as the employment data is continuous). The model fit statistics are then written into a .csv file that has been formatted to be accepted as universal input by the F-CAP R code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##### STEP 3: FIT THE LCMM MODELS AND OUTPUT MODEL FIT

# Create the file for running the procedure from 2-10 groups
batchit&amp;lt;-function(formula, groups, model){
  batch&amp;lt;-expand.grid(formula, rep(2:groups))
  colnames(batch)&amp;lt;-c(&amp;quot;formula&amp;quot;, &amp;quot;numberofgroups&amp;quot;)
  batch$link&amp;lt;-model
  batch$formula&amp;lt;-as.character(batch$formula)
  batch$numberofgroups&amp;lt;-as.numeric(as.character(batch$numberofgroups))
  return(batch)
}
batch&amp;lt;-batchit(&amp;quot;x&amp;quot;, 10, &amp;quot;linear&amp;quot;)

# Model Fit output function
#file.remove(list.files(&amp;quot;trajectories/fcap/input/&amp;quot;, pattern=&amp;quot;Universal_*&amp;quot;, full.names = T))
modelfittraj&amp;lt;-function(formula, numberofgroups, link){
  d4&amp;lt;-lcmm(as.formula(paste0(&amp;quot;y~&amp;quot;, formula)),
           as.formula(paste0(&amp;quot;mixture=~&amp;quot;, formula)),
           #as.formula(paste0(&amp;quot;random=~&amp;quot;, formula)),
           # whether to include random effects is quite a talking point
           subject=&amp;#39;id&amp;#39;,
           ng=numberofgroups,
           idiag=TRUE, 
           data=traj,
           link=link)
  postprob(d4)
  conv&amp;lt;-ifelse(d4$conv==1, &amp;quot;TRUE&amp;quot;, &amp;quot;FALSE&amp;quot;)
  k&amp;lt;-paste(d4$AIC, d4$BIC, exp(d4$loglik), conv)
  k&amp;lt;-gsub(&amp;quot; &amp;quot;, &amp;quot;,&amp;quot;, k)
  pp&amp;lt;-d4$pprob
  pp&amp;lt;-cbind(pp[,3:(ncol(pp))], d4$pprob$class)
  sink(paste0(&amp;quot;trajectories/fcap/input/&amp;quot;,formula, numberofgroups, link,&amp;quot;.txt&amp;quot;))
  cat(k, &amp;quot;\n&amp;quot;)
  # Stop writing to the file
  sink()
  write.table(pp, paste0(&amp;quot;trajectories/fcap/input/&amp;quot;,formula, numberofgroups, link,&amp;quot;.txt&amp;quot;), 
              append=TRUE, 
              sep = &amp;quot;,&amp;quot;,
              row.names = F,
              col.names = F)
  output&amp;lt;-read.table(paste0(&amp;quot;trajectories/fcap/input/&amp;quot;,formula, numberofgroups, link,&amp;quot;.txt&amp;quot;), row.names=NULL, sep=&amp;quot;,&amp;quot;, fill=T, header=F)
  file.remove(paste0(&amp;quot;trajectories/fcap/input/&amp;quot;,formula, numberofgroups, link,&amp;quot;.txt&amp;quot;))
  output[is.na(output)]&amp;lt;-&amp;quot;&amp;quot;
  readr::write_csv(output, paste0(&amp;quot;trajectories/fcap/input/&amp;quot;,&amp;quot;Universal_&amp;quot;, numberofgroups,&amp;quot;.csv&amp;quot;), col_names = F)
}
# Batch it
library(plyr)
#mdply(batch,modelfittraj)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then take the model fit output and use it as input to the F-CAP function that was supplied by Valeria Lima Passos (co-author of the paper above).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##### STEP 4 - Run the CSV files through F-CAP

#source(paste0(&amp;quot;trajectories/fcap/fcap_base_12_PC.R&amp;quot;), echo=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next step is to look at the F-CAPs and make a decision on the number of classes. Note, where there are + or - signs after some of the model fit statistics, this is the highest and lowest per group, as opposed to the mean. The first plot shows the AIC and BIC (penalises model complexity to a higher degree than AIC). Generally for these two, the lower the number the better the model fit. Six classes has the lowest BIC, but there could be an argument for 4 clases being the elbow.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;img/Traj_images/AICBIC.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The average posterior probability of assignment (APPA) shows the probability of belonging to a certain class. Generally, over 70% for APPA is defined as sufficient. The SD is standard deviation of group membership probabilities and is generally lower when model fit is better. The mismatch between estimated and assigned group probabilities (Mismatch) in a perfect model is zero. In this case, all of the classes have over 70%, with the highest probabilities for 2, then 4 and 5 classes. The mismatch is close to zero for all classes, with slight increasing trend with increasing number of classes. The SD is lowest for 2 groups and is noticebly higher after 5 classes.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;img/Traj_images/PP.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The Odds of Correct Classification (OCC) compares the average posterior probability assignment, correcting for OCC based on random assignment. Generally, higher OCC’s indicate better model fit with a threshold of 5 used to show good assignment accuracy. In this case, all classes have sufficient accuracy in class assignment but especially so for classes 3 to 9.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;img/Traj_images/OCC.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;We tend not to want classes that contain a very small number of units. A common threshold to apply is that each group will have at least 1% of the sample. However this should be modified if total number of units in the sample are low. In this case, although we are above the 1% threshold in all models with under 6 classes, because we have so few in the sample (N=32), a suitable number of classes would 2,3 or 4, given that at least ~10% of the sample are in each class.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;img/Traj_images/Small.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;In conclusion, the best number of classes in the model of LA employment trajectories from 2006-2015 is four. Although some of the criteria showed little variation between the classes (OCC, Mismatch), we arrived at as this number of classes using the following criteria:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BIC elbow&lt;/li&gt;
&lt;li&gt;High posterior probabilities (&amp;gt;90%)&lt;/li&gt;
&lt;li&gt;Relatively low standard deviation of PP within each class&lt;/li&gt;
&lt;li&gt;Had at least ~10% of the sample in any one class&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The next step is to then see whether changing the polynomial order gives any improvement in these model fit statistics (although in R I can only seem to change all the clusters at once rather than individually). For brevity, this is not shown in this document.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## What&amp;#39;s the best model?

##### STEP 5 - Finalise the group membership and plot

# plot traj
plottraj&amp;lt;-function(formula, numberofgroups, link, output){
  k&amp;lt;-NULL
  traj$id &amp;lt;- as.factor(traj$id)
  d4&amp;lt;-lcmm(as.formula(paste0(&amp;quot;y~&amp;quot;, formula)),
           as.formula(paste0(&amp;quot;mixture=~&amp;quot;, formula)),
           subject=&amp;#39;id&amp;#39;,
           ng=numberofgroups,
           idiag=TRUE, 
           data=traj,
           link=link)
  postprob(d4)
  traj$id2 &amp;lt;- as.numeric(traj$id)
  groups &amp;lt;- as.data.frame(d4$pprob[,1:2])
  traj$TRAJGROUP &amp;lt;- factor(groups$class[sapply(traj$id2, function(x) which(groups$id==x))])
if (output==&amp;quot;plot&amp;quot;){
ggplot(traj, aes(x, y, group=id, colour=group2)) + geom_line() + geom_smooth(aes(group=group2), method=&amp;quot;loess&amp;quot;, size=2, se=F) + labs(x=&amp;quot;x&amp;quot;,y=&amp;quot;y&amp;quot;,colour=&amp;quot;Latent Class&amp;quot;)
  pl&amp;lt;-ggplot(traj, aes(as.factor(x), y, group=id, colour=TRAJGROUP)) + geom_smooth(aes(group=id, colour=TRAJGROUP),size=0.5, se=F) + geom_smooth(aes(group=TRAJGROUP), method=&amp;quot;loess&amp;quot;, size=2, se=T) + labs(x=&amp;quot;Date&amp;quot;,y=&amp;quot;% Employed&amp;quot;,colour=&amp;quot;Latent Class&amp;quot;)
#  ggsave(paste0(&amp;quot;plots/&amp;quot;,formula, #numberofgroups, link,&amp;quot;.png&amp;quot;), plot=pl)
  pl

} else if (output==&amp;quot;file&amp;quot;){write.csv(traj, paste0(&amp;quot;data/&amp;quot;,formula, numberofgroups, link,&amp;quot;.csv&amp;quot;),row.names=F)}
}

# Put in the F-CAP best model here
plottraj(&amp;quot;x&amp;quot;, 4, &amp;quot;linear&amp;quot;, &amp;quot;plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Be patient, lcmm is running ... 
## The program took 1.48 seconds 
##  
## Posterior classification: 
##   class1 class2 class3 class4
## N   3.00  15.00   3.00  11.00
## %   9.38  46.88   9.38  34.38
##  
## Posterior classification table: 
##      --&amp;gt; mean of posterior probabilities in each class 
##         prob1  prob2  prob3  prob4
## class1 0.9993 0.0007 0.0000 0.0000
## class2 0.0000 0.9976 0.0000 0.0024
## class3 0.0000 0.0000 0.9998 0.0002
## class4 0.0000 0.0476 0.0005 0.9519
##  
## Posterior probabilities above a threshold (%): 
##          class1 class2 class3 class4
## prob&amp;gt;0.7    100    100    100  90.91
## prob&amp;gt;0.8    100    100    100  90.91
## prob&amp;gt;0.9    100    100    100  90.91
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/gbtm_files/figure-html/STEP5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The groups are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Group 1: Orkney Islands, Shetland Islands, Aberdeenshire&lt;/li&gt;
&lt;li&gt;Group 2: East Lothian, East Renfrewshire, Eilean Siar, Falkirk, Fife, Highland, Midlothian, Moray, Perth and Kinross, Scottish Borders, Aberdeen City, Argyll and Bute , West Lothian, Angus, East Dunbartonshire&lt;/li&gt;
&lt;li&gt;Group 3: North Ayrshire, Dundee City, Glasgow City&lt;/li&gt;
&lt;li&gt;Group 4: Clackmannanshire, Dumfries and Galloway, East Ayrshire, Inverclyde, South Ayrshire, South Lanarkshire, Stirling, City of Edinburgh, Renfrewshire, West Dunbartonshire, North Lanarkshire&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;n.b We do not include random effects in the above models. These are not included in the the SAS and Stata procedures for GBTM. The implication is that we do not allow variation within or between classes. The practical implication is that this will result in a greater number of latent classes being selected (Further discussion in &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3085403/&#34;&gt;Saunders&lt;/a&gt;, 2011).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Pathogen seasonality and links with weather in England and Wales: a big data time series analysis</title>
      <link>/publication/pathogen/</link>
      <pubDate>Tue, 28 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/pathogen/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Changing labour market conditions during the ‘great recession’ and mental health in Scotland 2007–2011: an example using the Scottish Longitudinal Study and data for local areas in Scotland</title>
      <link>/publication/sdai/</link>
      <pubDate>Mon, 13 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/sdai/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
