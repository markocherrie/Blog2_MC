<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Researcher on Researcher</title>
    <link>/</link>
    <description>Recent content in Researcher on Researcher</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Mark Cherrie</copyright>
    <lastBuildDate>Tue, 29 Aug 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>NetCDF in R</title>
      <link>/post/netcdf/</link>
      <pubDate>Sat, 24 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/netcdf/</guid>
      <description>&lt;p&gt;NetCDF stands for “Network Common Data Format” and was created by ‘unidata’ for handling large geospatial data. Here’s a short description from their &lt;a href=&#34;https://www.unidata.ucar.edu/netcdf/&#34;&gt;website&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;“NetCDF is a set of software libraries and self-describing, machine-independent data formats that support the creation, access, and sharing of array-orientated scientific data.”&lt;/p&gt;
&lt;p&gt;NetCDF files are containers for dimensions, variables and global atttributes. It’s used to store climatology, meteorology and oceanography data by over 1300 organisations including &lt;a href=&#34;https://www.esrl.noaa.gov/psd/data/gridded/data.gistemp.html&#34;&gt;NOAA&lt;/a&gt; and &lt;a href=&#34;https://www.eumetsat.int/website/home/index.html&#34;&gt;EUMETSAT&lt;/a&gt;. I think the key advantage of the format is that it is self-describing, whereby all the information required to use it correctly is supplied within the data file. So it doesn’t matter if you are using Python, Perl or R, you will be able to read, manipulate and plot the data, and there is lots of information out there to help with that.&lt;/p&gt;
&lt;p&gt;I was asked recently to help with processing air pollution data stored in NetCDF format (.nc/.NC) using R. I’ve had a first attempt at describing one of the files we are using (NO2 max for a day), plotting, batch processing daily neighbourhood (postcode unit) based exposure estimates and finally converting to .rds for upload to a postgresql database. Some considerations of the data I was using was that it had a polar stereographic projection, which can be converted to a lat/long &lt;a href=&#34;https://stackoverflow.com/questions/23837918/convert-polar-stereographic-projection-into-lat-long-grid-in-r&#34;&gt;grid&lt;/a&gt;. I decided that this might not be required if I used the points as they were and allocate the nearest air pollution point to the centroid of the postcode. As the data size was quite large the normal rgeos::gDistance method didn’t work (i.e. could not allocate vector error), so I used k-nearest neighbour (k=1), below is the code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# reading ncdf in R
library(ncdf4)
library(reshape2)
library(dplyr)
library(raster)

# get all netcdf files
flist &amp;lt;- list.files(path = &amp;quot;.&amp;quot;, pattern = &amp;quot;^.*\\.(nc|NC|Nc|Nc)$&amp;quot;)

# Open a connection to the first file in our list
nc &amp;lt;- nc_open(flist[1])

# get variable names
attributes(nc$var)$names

# summary
MaxNO2&amp;lt;-ncatt_get(nc, attributes(nc$var)$names[3])

#projection is:
#projection: Stereographic
#projection_params: 90.0 -32.0 0.933013
# got this from the nc output
nc_close(nc)

# use the info collected from attributes to get the data
xyznames &amp;lt;- c(&amp;quot;lon&amp;quot;, &amp;quot;lat&amp;quot;, &amp;quot;NO2_daymax&amp;quot;)
lon &amp;lt;- raster(flist[1], varname= xyznames[1])
lat &amp;lt;- raster(flist[1], varname= xyznames[2])
dat &amp;lt;- brick(flist[1], varname = xyznames[3])

# Values drops the &amp;quot;raster&amp;quot; wrapper, just returns values in order as a vector
d &amp;lt;- cbind(values(lon), values(lat), values(dat[[1]]))
# remap arbitrary values to [0,1] for a colour table
scl &amp;lt;- function(x) (x - min(x, na.rm = TRUE))/diff(range(x, na.rm = TRUE))
# set a number for colours
n &amp;lt;- 56
# plot the data
plot(d[,1:2], pch = 16, col = terrain.colors(n)[scl(d[,3]) * (n-1) + 1])

# codepoint postcode unit points
set1&amp;lt;-readRDS(&amp;quot;C:/Users/mcherrie/boundary/Postcodeunit.rds&amp;quot;)
set1 &amp;lt;- spTransform(set1, CRS(&amp;quot;+init=epsg:4326&amp;quot;))

# raster vals
coords = cbind(d[,1], d[,2])
set2 = SpatialPoints(coords)
proj4string(set2) = CRS(&amp;quot;+init=epsg:4326&amp;quot;)
plot(set2)

A &amp;lt;- SpatialPoints(set1)
B &amp;lt;- SpatialPoints(set2)
# library(rgeos)
# set1$nearest_in_set2 &amp;lt;- apply(gDistance(set1sp, set2sp, byid=TRUE), 1, which.min)
# doesn&amp;#39;t work cannot allocate vector

library(SearchTrees)
# Find indices of the nearest points in B to each of the points in A
tree &amp;lt;- createTree(coordinates(B))
inds &amp;lt;- knnLookup(tree, newdat=coordinates(A), k=1)

# create the dataframes --- need to double check this bit!
Bdf&amp;lt;-as.data.frame(B)
Bdf$ind&amp;lt;-seq(1,nrow(Bdf))
colnames(Bdf)&amp;lt;-c(&amp;quot;AP_long&amp;quot;, &amp;quot;AP_lat&amp;quot;, &amp;quot;ind&amp;quot;)
Adf&amp;lt;-as.data.frame(A)
Adf$ind&amp;lt;-inds
colnames(Adf)&amp;lt;-c(&amp;quot;PU_long&amp;quot;, &amp;quot;PU_lat&amp;quot;, &amp;quot;ind&amp;quot;)
Cdf&amp;lt;-merge(Adf, Bdf, by=&amp;quot;ind&amp;quot;)

# get the Air pollution coordinates for the Postcode units
PUDF&amp;lt;-as.data.frame(set1)
colnames(PUDF)&amp;lt;-c(&amp;quot;PU&amp;quot;, &amp;quot;Easting&amp;quot;, &amp;quot;Northing&amp;quot;, &amp;quot;PU_long&amp;quot;, &amp;quot;PU_lat&amp;quot;)
PUDF2&amp;lt;-merge(PUDF, Cdf, by=c(&amp;quot;PU_long&amp;quot;, &amp;quot;PU_lat&amp;quot;), all.x=T)
subsetAP&amp;lt;-subset(PUDF2, select=c(&amp;quot;PU&amp;quot;, &amp;quot;AP_long&amp;quot;, &amp;quot;AP_lat&amp;quot;))

# Let&amp;#39;s output a year
for(i in 1:2){
d &amp;lt;- cbind(values(lon), values(lat), values(dat[[i]]))
name&amp;lt;-dat[[i]]@z[[1]][1]
colnames(d)&amp;lt;-c(&amp;quot;AP_long&amp;quot;, &amp;quot;AP_lat&amp;quot;, &amp;quot;Value&amp;quot;)
d2&amp;lt;-merge(d, subsetAP, by=c(&amp;quot;AP_long&amp;quot;, &amp;quot;AP_lat&amp;quot;), all.y=T)
saveRDS(d2, paste0(&amp;quot;E:/airpollutiondata/&amp;quot;,name,&amp;quot;.rds&amp;quot;))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Any feedback welcome, thanks.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Life course of place: a longitudinal study of mental health and place</title>
      <link>/publication/lifecourseplacemental/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/lifecourseplacemental/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A randomised control crossover trial of a theory based intervention to improve sun-safe and healthy behaviours in construction workers: study protocol</title>
      <link>/publication/nudgeuv/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/nudgeuv/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Nudging people to make healthy behaviour choices</title>
      <link>/post/rshiny-weather-app/</link>
      <pubDate>Fri, 22 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/rshiny-weather-app/</guid>
      <description>&lt;p&gt;Nudge theory made it big after the 2008 book by Thaler and Sunstein gathered some high profile readers (Obama, Cameron etc). Alongside public health officials and politicians, researchers have been interested in using the theory to influence people to make ‘healthy’ behaviour choices. The central tenant of the theory is based around libertarian paternalism, or influencing an individual’s ‘choice architecture’ without coercion. Nudge-type interventions (e.g. nutritional labeling, size of serving dishes) have been used to tackle &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4967524/&#34;&gt;obesity&lt;/a&gt; and were found to increase ‘healthy’ behaviours by 15.3%.&lt;/p&gt;
&lt;p&gt;Biometerology is the study of the impacts of the weather and climate on human health. The Met Office provide a cold weather &lt;a href=&#34;https://www.metoffice.gov.uk/public/weather/cold-weather-alert/#?tab=coldWeatherAlert&#34;&gt;alert&lt;/a&gt; to ensure that people with conditions susceptible to the cold (e.g. COPD) can take precautions, based on evidence of spikes in hospital admissions in weeks of &lt;a href=&#34;http://err.ersjournals.com/content/15/101/185.full&#34;&gt;cold weather - below 0°C&lt;/a&gt;. This kind of thing seems to be more popoular, with Facebook recently implementing providing weather information on their users’ timelines. I wondered whether I could replicate this type of nudge intervention in an R Shiny application.&lt;/p&gt;
&lt;p&gt;In terms of data sources, the &lt;a href=&#34;https://www.metoffice.gov.uk/datapoint&#34;&gt;datapoint API&lt;/a&gt; available from the UK Met Office provides 3 hourly forecasts for a range of weather (e.g. temperature, UV, wind speed) for 5,000 UK sites (i.e. requires a script to determine closest station). Data from the Met Office goes through many rigorous checks and is reliable. Functions to query the datapoint API have been written in &lt;a href=&#34;https://pypi.python.org/pypi/datapoint/&#34;&gt;python&lt;/a&gt; and porting this to R is not a small task. Also, the datapoint API has been taken offline in recent months so that it can be updated (UPDATE 02/02/2018, it is now back online, and will be used in the future).&lt;/p&gt;
&lt;p&gt;In the interim I used weather data from the &lt;a href=&#34;https://www.wunderground.com/weather/api/&#34;&gt;Weather Underground API&lt;/a&gt;. This provides hourly weather forecasts for a given latitude and longitude and is available for locations around the world. The latitude and longitude was determined by the user’s smartphone or computer &lt;a href=&#34;https://github.com/AugustT/shiny_geolocation&#34;&gt;location&lt;/a&gt;. As this is a test, and I want to do a full literature review to ensure the best weather thresholds, I started with a trivial example - whether it is warm enough to wear shorts to work (arbitrarily set at over 15°C).&lt;/p&gt;
&lt;p&gt;Below is the code for the UI:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(shiny)
library(rwunderground)
shinyUI(fluidPage(
  titlePanel(&amp;quot;Shorts to work?&amp;quot;),
    tags$script(&amp;#39;
                $(document).ready(function () {
                navigator.geolocation.getCurrentPosition(onSuccess, onError);
                
                function onError (err) {
                Shiny.onInputChange(&amp;quot;geolocation&amp;quot;, false);
                }
                
                function onSuccess (position) {
                setTimeout(function () {
                var coords = position.coords;
                console.log(coords.latitude + &amp;quot;, &amp;quot; + coords.longitude);
                Shiny.onInputChange(&amp;quot;geolocation&amp;quot;, true);
                Shiny.onInputChange(&amp;quot;lat&amp;quot;, coords.latitude);
                Shiny.onInputChange(&amp;quot;long&amp;quot;, coords.longitude);
                }, 1100)
                }
                });
                &amp;#39;),
    
    # output message
    fluidRow(column(width = 2,
                    textOutput(&amp;quot;message&amp;quot;))
    ),
  tags$head(tags$style(HTML(&amp;quot;
                            #message {
                            font-size: 100px;
                            }
                            &amp;quot;)))
  
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the code for the server:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(shiny)
library(rwunderground)

shinyServer(function(input, output) {
  observe({
    if(!is.null(input$lat)){
    output$message &amp;lt;- renderText({
      location&amp;lt;-as.character(paste0(input$lat,&amp;quot;,&amp;quot;, input$long))
      ifelse(hourly(set_location(lat_long = location ), use_metric = TRUE, key = &amp;quot;&amp;quot;, raw = FALSE, message = FALSE)[2,2]&amp;gt;15, &amp;quot;YES&amp;quot;, &amp;quot;NO&amp;quot;)
      })
    }
      })
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the final app:&lt;/p&gt;
&lt;iframe src=&#34;https://thebest.shinyapps.io/shortstowork/?showcase=0&#34; width=&#34;672&#34; height=&#34;600px&#34;&gt;
&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Mapping Green Space and Traffic in Glasgow</title>
      <link>/post/greentraffic/</link>
      <pubDate>Fri, 10 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/greentraffic/</guid>
      <description>&lt;p&gt;The question, in relation to theories of how the quality of greenspace can affect relationships with health, might be: Do traffic events occur close to greenspaces? How does this vary spatially and temporally? How can this be visualised?&lt;/p&gt;
&lt;p&gt;First load the packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Load packages
# allows r to work with spatial data
library(rgdal)
# allows r to connect with google apis
library(googleway)
# allows r to perform GIS operations (on vectors especially?)
library(rgeos)
# general purpose plotting package for r
library(ggplot2)
# download the greenspace map for scotland from:
# https://www.ordnancesurvey.co.uk/opendatadownload/products.html&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, read in the green space data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# read the data
green&amp;lt;-readOGR(&amp;quot;boundaries/green&amp;quot;, &amp;quot;GB_GreenspaceSite&amp;quot;)
# Download the local authority boundary for Glasgow City from here:
# https://saspac.org/data/2011-census/scotland-2011/
glasgow&amp;lt;-readOGR(&amp;quot;boundaries/Scottish-Census-boundaries(shp)/2011_Council_Area/CA_2011_EoR_Glasgow_City&amp;quot;, &amp;quot;CA_2011_EoR_Glasgow_City&amp;quot;)
## OGR data source with driver: ESRI Shapefile 
green_subset &amp;lt;- green[glasgow, ]

# Project Glasgow greenspace (BNG) to lat/long (WGS84)
green_subset &amp;lt;- spTransform(green_subset, CRS(&amp;quot;+init=epsg:4326&amp;quot;))

# Make it a dataframe
green_polygon&amp;lt;-fortify(green_subset)

# Tidy the coloumn names so that we can use it with the polyline encoding function
colnames(green_polygon)[1]&amp;lt;-&amp;quot;lon&amp;quot;
green_polygon&amp;lt;-subset(green_polygon, select=c(&amp;quot;lon&amp;quot;, &amp;quot;lat&amp;quot;, &amp;quot;id&amp;quot;))

# Now for each group of points (vertices of polygon) we want to encode the polygon using the &amp;quot;encode_pl&amp;quot; function from the package googleway (comment block it do it doens&amp;#39;t run everytime)

if(FALSE) {
polyline_df&amp;lt;-NULL
for (i in unique(green_polygon$id)){
dataG&amp;lt;-subset(green_polygon, id==i)
polyline&amp;lt;-encode_pl(dataG$lat, dataG$lon)
polyline_df&amp;lt;-rbind(df, polyline)
}
polyline_df&amp;lt;-as.data.frame(polyline_df)
colnames(polyline_df)&amp;lt;-&amp;quot;polyline&amp;quot;
}
}
# Let&amp;#39;s write this out so that the end viz/application doesn&amp;#39;t need to do this bit again
# write.csv(polyline_df, &amp;quot;data/polyline_df.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s get historic traffic data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Data on historical traffic from here:
# https://www.dft.gov.uk/traffic-counts/area.php?region=Scotland&amp;amp;la=Glasgow+City
# &amp;quot;Briefly the file contains the annual traffic (otherwise known as volume of traffic) on each link of the major road network. This is calculated by multiplying the Annual average daily flow (AADF) by the corresponding length of road and by the number of days in the year&amp;quot;&amp;quot;
data&amp;lt;-read.csv(&amp;quot;data/Glasgow+City_TRAF.csv&amp;quot;)
# make the data frame spatial
coords = cbind(data$Easting, data$Northing)
sp = SpatialPoints(coords)
# add the data
spdf = SpatialPointsDataFrame(coords, data)
# Set projection to bng
proj4string(spdf) &amp;lt;- CRS(&amp;quot;+init=epsg:27700&amp;quot;)
# project to bng
spdf &amp;lt;- spTransform(spdf, CRS(&amp;quot;+init=epsg:4326&amp;quot;))
# Let&amp;#39;s just look at all the motovehicles combineds
traffichistoric&amp;lt;-as.data.frame(spdf)
traffichistoric&amp;lt;-subset(traffichistoric, select=c(&amp;quot;coords.x1&amp;quot;, &amp;quot;coords.x2&amp;quot;, &amp;quot;AllMotorVehicles&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can get the relevant information from the googleway package, i.e. current traffic volume. So all that’s needed is to put it in a shiny app for the final data visualisation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load packages
library(shiny)
library(googleway)
library(maptools)

# Read the processed data
traffichistoric&amp;lt;-read.csv(&amp;quot;data/traffichistoric.csv&amp;quot;)
polyline_df&amp;lt;-read.csv(&amp;quot;data/polyline_df.csv&amp;quot;)

# Create the shiny user input specs
ui &amp;lt;- fluidPage(google_mapOutput(&amp;quot;map&amp;quot;))

# Create the shiny server specs
server &amp;lt;- function(input, output, session){

# Have to have an API key to use Google data  
map_key &amp;lt;- &amp;quot;Your-key-here&amp;quot;
  
# Style option not used but we can change the style of the map with the following code:
style &amp;lt;- &amp;#39;[{&amp;quot;featureType&amp;quot;:&amp;quot;all&amp;quot;,&amp;quot;elementType&amp;quot;:&amp;quot;all&amp;quot;,&amp;quot;stylers&amp;quot;:[{&amp;quot;invert_lightness&amp;quot;:true}, {&amp;quot;saturation&amp;quot;:10},{&amp;quot;lightness&amp;quot;:30},{&amp;quot;gamma&amp;quot;:0.5},{&amp;quot;hue&amp;quot;:&amp;quot;#435158&amp;quot;}]}, {&amp;quot;featureType&amp;quot;:&amp;quot;road.arterial&amp;quot;,&amp;quot;elementType&amp;quot;:&amp;quot;all&amp;quot;,&amp;quot;stylers&amp;quot;:[{&amp;quot;visibility&amp;quot;:&amp;quot;simplified&amp;quot;}]}, {&amp;quot;featureType&amp;quot;:&amp;quot;transit.station&amp;quot;,&amp;quot;elementType&amp;quot;:&amp;quot;labels.text&amp;quot;,&amp;quot;stylers&amp;quot;:[{&amp;quot;visibility&amp;quot;:&amp;quot;off&amp;quot;}]}]&amp;#39;
  
# output the map, use glasgow lat long to centre map, play with the zoom to make it look good
output$map &amp;lt;-renderGoogle_map({google_map(key = map_key, location=c(55.8642, -4.2518), zoom=13) %&amp;gt;%
# Add a heatmap of the historical traffic counts
add_heatmap(data = traffichistoric, lat = &amp;#39;coords.x2&amp;#39;, lon = &amp;#39;coords.x1&amp;#39;, option_opacity = 0.3) %&amp;gt;%
# This is the key bit to get contemporary traffic
add_traffic() %&amp;gt;% 
# And to the get the polygons of greenspace
add_polygons(data = polyline_df, polyline =&amp;quot;polyline&amp;quot;, stroke_weight = 3, fill_colour = &amp;quot;green&amp;quot;, stroke_colour = &amp;quot;black&amp;quot;) 
})}
# run the app!
shinyApp(ui, server)&lt;/code&gt;&lt;/pre&gt;
&lt;iframe src=&#34;https://thebest.shinyapps.io/test_MC/?showcase=0&#34; width=&#34;672&#34; height=&#34;600px&#34;&gt;
&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Green space and cognitive ageing: A retrospective life course analysis in the Lothian Birth Cohort 1936</title>
      <link>/publication/parkscogage/</link>
      <pubDate>Mon, 06 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/parkscogage/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Removing pages from a PDF</title>
      <link>/post/pdf/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/pdf/</guid>
      <description>&lt;p&gt;More of a note than a post but hopefully this can help someone else out having the same issue.&lt;/p&gt;
&lt;p&gt;I needed to remove a couple of pages from a PDF document today. This can be done easily on a mac using preview (i.e. backspace when the page thumbnail is highlighted) but I couldn’t find a way on the PC, using my default PDF reader- Acrobat Reader DC. It seems that most of the useful functionality requires the pro version.&lt;/p&gt;
&lt;p&gt;I first tried to edit it in GIMP (opensource photo editing software). I was able to import each of the PDF pages as layers and delete the layers I didn’t need. However, I couldn’t find a way of exporting properly, all I got was the first page. I did see something about saving it as a .mng and using inkscape/imagemagick’s ‘convert’ command but it seemed like a bit of work. Next up, LibreOffice, I saw an article describing that it was possible to import and edit a PDF using this software. This worked well, and was very intuitive, however when I looked at the exported PDF document it had made all the pages portrait. This was a problem as my some of my pages with wide tables were landscape, so it had clipped them. Not being able to find a way to rotate individual pages in LibreOffice (there must be a way of doing this!), I tried a final option to use the Google Drive extension - DocHub. This worked so well and made it really easy to save the edited pdf back to my Google Drive.&lt;/p&gt;
&lt;p&gt;So if you need some powerful software to edit a PDF quickly, check out &lt;a href=&#34;https://dochub.com&#34;&gt;DocHub&lt;/a&gt;! Just beware that it’s only free up to a point. You can view and edit up to 2,000 documents, sign 5 documents/month, request up to 3 signers/month and email up to 3 documents/day, anything over this and you will need to go onto the pro account.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data, data, big not small, how can R increase efficiency for all?</title>
      <link>/talk/example-talk/</link>
      <pubDate>Thu, 28 Sep 2017 11:30:00 +0000</pubDate>
      
      <guid>/talk/example-talk/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mapzen Isochrone Maps</title>
      <link>/post/isochrone/</link>
      <pubDate>Thu, 28 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/isochrone/</guid>
      <description>&lt;p&gt;I have just come across Mapzen’s routing engine (Valhalla), which can be used to produce incredible isochrone maps, mentioned in this &lt;a href=&#34;https://mapzen.com/blog/introducing-isochrone-service/&#34;&gt;blog post&lt;/a&gt;. If you’ve not heard of &lt;a href=&#34;https://mapzen.com/&#34;&gt;Mapzen&lt;/a&gt; they provide mapping tools for developers so that their websites can be converted to the spatial side. The neat thing about isochrone lines is that they show the boundary around a point where travel time is equal. So you can ask some interesting proximity based questions (e.g. What types of greenspace do I have within a 5 minute walk from my home?). I’ve been eager to create isochrone maps since seeing &lt;a href=&#34;https://www.rome2rio.com/blog/2016/01/08/time-flies-according-to-these-maps-it-does/&#34;&gt;Rome2Rio’s set&lt;/a&gt;, which show isochrones from London for 1914 and 2016. Below I have used the &lt;a href=&#34;https://mapzen.com/mobility/explorer/&#34;&gt;mobility explorer&lt;/a&gt; to generate isochrones boundaries (in 15 minute bands) for where I live, for several different travel options (walking, cycling, public transport and driving). Have a go!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/mapzenimages/walking.png&#34; alt=&#34;Walking&#34; /&gt; &lt;img src=&#34;img/mapzenimages/cycling.png&#34; alt=&#34;Cycling&#34; /&gt; &lt;img src=&#34;img/mapzenimages/transit.png&#34; alt=&#34;Public Transport&#34; /&gt; &lt;img src=&#34;img/mapzenimages/driving.png&#34; alt=&#34;Driving&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As with all my posts, I’m writing from memory, from my own experiences, which may have not been remembered correctly or do not include the full picture. If you think there should be an improvement to the post then please get in touch and I’ll update it, I’m always willing to learn!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Land cover and air pollution are associated with asthma hospitalisations: A cross-sectional study.</title>
      <link>/publication/asthmagreenairpoll/</link>
      <pubDate>Sat, 16 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/asthmagreenairpoll/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DISPLAY</title>
      <link>/project/display/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/display/</guid>
      <description></description>
    </item>
    
    <item>
      <title>First post</title>
      <link>/post/first-post/</link>
      <pubDate>Thu, 07 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/first-post/</guid>
      <description>&lt;p&gt;Welcome to my first post. As an avid R user, I was amazed at how quickly it was to setup a static website with the help from the R package: &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;“blogdown”&lt;/a&gt;. I thought Yihui Xie gave a convincing argument for blogging from his personal experience (Appendix D in above website). In summary he provides three reasons in favour of blogging:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;To consolidate thoughts and create a clearer argument for topics that are important to you.&lt;/li&gt;
&lt;li&gt;To engage with other people and get helpful feedback.&lt;/li&gt;
&lt;li&gt;To remember things that you have done so that they can be re-used in new applications.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These are things that I already try to do, albeit unsystematically. I find that the things I do in this respect are either quite time consuming (e.g. giving a tutorial or presentation) or fleeting (e.g. conversations with friends and family). With respect to the third point I have a github repository for &lt;a href=&#34;https://github.com/markocherrie/Helpful_Code&#34;&gt;helpful code&lt;/a&gt;, however this tends to be more of a dump of code snippets rather than a thoughtful write-up of what I was doing. So this website fits in nicely.&lt;/p&gt;
&lt;p&gt;I mentioned that this is a static website in the first paragraph. This terminology was completely new to me when I started to set this site up. A static website is one that has every webpage stored as a single file on the server, e.g. this post has it’s own HTML (Hypertext Markup Language) file associated with it. Hugo, Jeykll and Hexo are the static site generators that you pass these files to. The main advantage to static websites is that you can hit the ground running, a little bit of HTML and CSS (Cascading Style Sheets) knowledge can go a long way. There is no requirement to know any server side scripting languages such a PHP (Hypertext Preprocessor) or SQL (Structured Query Language), which means there is no server-host interaction and therefore load times are very quick. There are drawbacks though. If you intend to change something on every page, then this can be very time consuming. There is also a lot less functionality comapared to a dynamic page which gives you the ability to add features such as shopping carts and social media interactions. Another option would have been to use a CMS (Content Management System) such as Joomla or Wordpress, beating both static and dynamic on ease of setup and time to update. Although I feel that their functionality is a black box and if you want to deviate from the default template then you may get unstuck. CMS’s have got around this by adding plugins. However I have found that often these aren’t available on the cheaper pricing tiers. For example Wordpress does not allow you to use iframe (a webpage inside another webpage…webception) and adding a plugin is only available on the business pricing tier (£20.83 a month). So I’d argue that a static website for a blog is a good option, which allows the website to grow as the webmaster’s skills do.&lt;/p&gt;
&lt;p&gt;As with all my posts, I’m writing from memory, from my own experiences, which may have not been remembered correctly or do not include the full picture. If you think there should be an improvement to the post then please get in touch and I’ll update it, I’m always willing to learn!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alcohol and Tobacco Environments in Scotland</title>
      <link>/project/webmap/</link>
      <pubDate>Tue, 29 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/webmap/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Analysing the Association between Ultraviolet Radiation, Vitamin D and Allergic Disease</title>
      <link>/project/uv/</link>
      <pubDate>Tue, 29 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/uv/</guid>
      <description>&lt;p&gt;Exposure to vitamin D effective ultraviolet radiation (UVvitd) is key to the
production of vitamin D in human skin. Ambient ultraviolet radiation, dietary intake
of D3/D2 and 25-hydroxy vitamin D in serum and saliva have been used as a proxy
for vitamin D status. However, these exposures have yet to yield a consistent
effect on allergic diseases. The aim of the thesis was to determine the spatiotemporal
distribution of UVvitd and 25(OH)D in Britain, so as to investigate the
effect on allergic health outcomes.&lt;/p&gt;

&lt;p&gt;The relative effectiveness of known determinants of UVvitd and 25(OH)D
(latitude and month of the year) and proposed determinants (longitude, elevation,
urban/rural residence, coastal residence, year of blood measurement, day of
blood measurement/year) were described. Coastal residence was shown to
be a novel geographic determinant of higher levels of UVvitd and 25(OH)D in
England and may help to explain previous associations with general health and
wellbeing. Further to these results, the relationship between UVvitd and 25(OH)D
was investigated. This stimulated the development of an instrumental variable,
which can be used to reduce the effect of confounding in observational studies.
Common meteorological data including sunshine duration and temperature were
used to predict ambient UVvitd at a postcode sector level. The ambient UVvitd
instrument was shown to linearly predict 25(OH)D, measured in the 1958 Birth
Cohort participants aged 45, at three and six month cumulative doses before
blood measurement. Effectiveness was attenuated by the duration of outdoor
activity. The relationship between ambient UVvitd and 25(OH)D was used to
estimate the effect of maternal 25(OH)D on hay fever, eczema and asthma in
childhood. Consistent positive associations were found with hay fever only. There
may have been beneficial differential effect on asthma by atopic status and sex
of the participant. In males with hay fever asthma prevalence was lowest in the
highest UVvitd quartile. However the marker of atopy was weak (i.e. parental
reported hay fever) and the interaction was insignificant (i.e. p=0.07). Therefore,
the association between 25(OH)D and asthma prevalence and incidence was
investigated, with atopic status determined using a mixture of self-reported and
biomedical data (i.e. IgE measurements) from the 1958 Birth Cohort in adulthood.
The interaction between 25(OH)D and atopic status was highly significant and non-atopic individuals were shown to have an increased risk of asthma for
every 10 nmol/l decrease in 25(OH)D. Given that vitamin D represents only one
health effect of human interactions with climate, the relationship between several
meteorological variables and GP prevalence rates of asthma was investigated.
A combination of high UVvitd and temperature and low relative humidity and
precipitation was associated with lower asthma prevalence.
To summarise, the findings have determined that UVvitd varies geographically
and temporally with 25(OH)D in Britain and higher levels could benefit specific
subtypes of allergic disease. By utilising multiple existing meteorological data,
the understanding of the aetiology of allergic disease can be advanced.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Environmental determinants of IVF treatment</title>
      <link>/project/ivf/</link>
      <pubDate>Tue, 29 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/ivf/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
