<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Javascript on Mark Cherrie</title>
    <link>http://www.markcherrie.net/tags/javascript/</link>
    <description>Recent content in Javascript on Mark Cherrie</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2020 Mark Cherrie</copyright>
    <lastBuildDate>Thu, 26 Apr 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/javascript/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Google Earth Engine: NDVI in Scotland 2001-2017</title>
      <link>http://www.markcherrie.net/post/gee/</link>
      <pubDate>Thu, 26 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.markcherrie.net/post/gee/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;div id=&#34;motivation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Motivation&lt;/h4&gt;
&lt;p&gt;I’m interested in a long-term small-area level measure of ‘green spaces’, features of the natural environment that are important for a wide variety of health outcomes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;green-spaces-over-time&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Green spaces over time&lt;/h4&gt;
&lt;p&gt;There are two major green space datasets for Scotland, &lt;a href=&#34;http://www.greenspacescotland.org.uk/scotlands-greenspace-map.aspx&#34;&gt;Scotland’s Greenspace Map 2011&lt;/a&gt; and the &lt;a href=&#34;https://www.ordnancesurvey.co.uk/business-and-government/products/os-open-greenspace.html&#34;&gt;Ordnance Survey’s Greenspace Map 2017&lt;/a&gt;. These maps were produced by characterising OS polygons using aerial photos, with the latter updated every 6 months from July 2017. They use the &lt;a href=&#34;https://beta.gov.scot/publications/planning-advice-note-pan-65-planning-open-space/documents/0060935.pdf&#34;&gt;PAN65&lt;/a&gt; criteria to categorise all Mastermap polygons into 11 types (e.g. Private gardens or grounds) and 23 land cover classifications (e.g. School grounds). Although change in green space types over time could be undertaken due to use of the same categories, there is no time point before 2011, which inhibits the utility to a number of cohort studies (e.g. Millenium cohort study). Furthermore, the 2011 data only covers settlements with over 3,000 people so would not give an estimate of change for people living in rural areas.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;land-cover-over-time&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Land cover over time&lt;/h4&gt;
&lt;p&gt;There are a number of global, national and continental land cover datasets available over time. There are global land cover datasets from &lt;a href=&#34;https://neo.sci.gsfc.nasa.gov/view.php?datasetId=MCD12C1_T1&#34;&gt;NASA&lt;/a&gt; for 2001-2011 and &lt;a href=&#34;http://due.esrin.esa.int/page_globcover.php&#34;&gt;ESA&lt;/a&gt; covering 2 periods: December 2004 - June 2006 and January - December 2009. Although the classifications for the global datasets seem quite broad, possibly due to the low spatial resolution. The Centre for Hydrology and Ecology have &lt;a href=&#34;https://www.ceh.ac.uk/services/information-products&#34;&gt;datasets&lt;/a&gt; for 1990, 2000, 2007 and 2015 on UK land cover but unfortunatley they can’t be compared between years due to different sampling methods being applied. Finally, there is data in between UK and global level called the &lt;a href=&#34;https://land.copernicus.eu/pan-european/corine-land-cover&#34;&gt;CORINE land cover series&lt;/a&gt;, created for 1990, 2000, 2006 and 2012. Land cover is &lt;a href=&#34;https://wiki.openstreetmap.org/wiki/Corine_Land_Cover&#34;&gt;classified&lt;/a&gt; into 5 categories and 44 classes. Change from 2006 and 2012 has been undertaken by researchers from the university of Leicester, available &lt;a href=&#34;https://catalogue.ceh.ac.uk/documents/35fecd0f-b466-448b-94d1-0bba90be450e&#34;&gt;here&lt;/a&gt;. Whilst CORINE data will allow the analysis of subtypes of green space, there is some evidence that total green space may be more important for health outcomes (Richardson et al., 2018; Klompmaker et al., 2018). Thus measures of vegetation levels based on satellite products may be appropriate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;vegetation-levels-over-time&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Vegetation levels over time&lt;/h4&gt;
&lt;p&gt;Normalised difference vegetation index (NDVI) is a measure of vegetation greeness and photosynthetic capacity. NDVI is calculated by an equation using estimates of red (Red; 0.2-0.7µm) and near-infrared (NIR; 0.7-1.1µm) radiation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[NDVI = {\frac{NIR - Red} {NIR + Red}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Green vegetation contains chlorophyll which absorbs more red light and reflects more near-infrared light. NDVI values range from -1 to 1, with values near zero as bare soil and 0.8 as temperate rainforests. Instruments, for example the Moderate Resolution Imaging Spectroradiometer (MODIS) aboard Landsat 7 (1999-) and Terra (1999-) record surface reflectance in the NIR and Red bands. The spatial resolution of Landsat 7 is 30 m but only images a scene (185 km long and wide) every 16 days. Terra has a 250 m resolution but images a scene every day. Both datasets (‘Landsat 7 ETM+ C1 Level-2’ and ‘MOS13Q1 V6’) can be acquired from the &lt;a href=&#34;https://earthexplorer.usgs.gov&#34;&gt;USGS EarthExplorer website&lt;/a&gt;. It should be noted that 22% of any scene after 31st of May 2003 is invalid due to a problem with the Scan Line Corrector, which compensates for the forward motion of Landsat 7, of which the following code can provide a &lt;a href=&#34;https://gis.stackexchange.com/questions/264061/ls7-filling-the-gaps-image-with-google-earth-engine&#34;&gt;fix&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;google-earth-enginegee&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Google Earth Engine(GEE)&lt;/h4&gt;
&lt;p&gt;The USGS explorer is a Graphical User Interface; it is time consuming to download all the images corresponding to large study areas. An &lt;a href=&#34;https://earthexplorer.usgs.gov/inventory/documentation/json-api&#34;&gt;application programming interface&lt;/a&gt;(API) also exists, so some data could be downloaded automatically. However once data has been downloaded, images would still need to be &lt;a href=&#34;https://www.nceas.ucsb.edu/scicomp/usecases/createrasterimagemosaic&#34;&gt;mosaiced&lt;/a&gt; together to undertake analysis at a national level. An alternative is to use the &lt;a href=&#34;https://earthengine.google.com&#34;&gt;Google Earth Engine&lt;/a&gt;, which provides a computing platform that allows users to run geospatial analysis on Google’s infrastructure. In other words, the massive remote sensing datasets required for large scale analyses are easily downloaded via a web API and mosaiced instantly using Google’s high performance parallel computation service. Analysis is performed using an interactive development environment (IDE), which lends itself to quick prototyping and visualisation of results. Crucially, any analysis can be scaled to another country or at a global level very easily. Some challenges arise from GEE’s terminology (e.g. mapping = applying functions) and client/server programming model (similar to Rshiny)(i.e. you cannot use external operations (loop/conditionals) on Earth engine proxy objects (‘.ee’)), although there are &lt;a href=&#34;https://gis.stackexchange.com/questions/247297/start-a-loop-from-information-in-a-feature-property/247319?utm_medium=organic&amp;amp;utm_source=google_rich_qa&amp;amp;utm_campaign=google_rich_qa&#34;&gt;ways&lt;/a&gt; around this. A full discussion on the utility of GEE for analysis is available (Gorelick et al., 2017).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method&lt;/h2&gt;
&lt;p&gt;The aim was to use all the available Landsat 7 images (2001-2017; image every 16 days), masking clouds and water, and calcuate the median NDVI value for all 30 m pixels within a 500 m radius of the datazone’s population weighted centroid.&lt;/p&gt;
&lt;div id=&#34;creating-kml-boundary-files&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Creating KML boundary files&lt;/h4&gt;
&lt;p&gt;First, download the Datazone 2011 &lt;a href=&#34;https://data.gov.uk/dataset/8aabd120-6e15-41bf-be7c-2536cbc4b2e5/data-zone-centroids-2011&#34;&gt;population weighted centroids&lt;/a&gt;, load into R and export as a .kml file.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gee-layout&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;GEE layout&lt;/h4&gt;
&lt;p&gt;Sign into Google Earth Engine with your gmail account. The GEE layout is as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/GEE_images/GEElayout.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The main panel here is the Code editor which will allow you to save code (to the script manager) and run a script.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;loading-vectors&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Loading vectors&lt;/h4&gt;
&lt;p&gt;Next sign into Google Fusion, click File&amp;gt;new table, then when the new window appears, Choose File&amp;gt;Next and follow the instructions. Note that the maximum size of file is 250 MB (if bigger then split up). When the table has loaded successfully, then click File&amp;gt;About this Table. Copy the Id. Now in GEE, in the Code Editor section, use the ‘ee.FeatureCollection’ function with Id you have copied to load the datazone boundaries.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-vectors&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Creating vectors&lt;/h4&gt;
&lt;p&gt;Note that you can define features as well by using the ‘ee.Feature’ function. There is the also the option to use the map directly. Clicking on the ‘Add a marker’ icon in the top left hand corner of the Map will import come code to the top of your code editor. If you have several features you can group them using the ‘ee.FeatureCollection’ function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;loading-rasters&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Loading rasters&lt;/h4&gt;
&lt;p&gt;We can search for GEE datasets using the search bar at the top of the GEE window. You can click the import icon, or copy the ImageCollection ID and use with the ‘ee.ImageCollection’ function. We can also import single images using ‘ee.Image’, as we have done to acquire the GEE dataset by Hansen et al., 2013.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mapping&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Mapping&lt;/h4&gt;
&lt;p&gt;GEE allows you to apply lots of GIS functions over collections of features (i.e. ‘ee.FeatureCollection’) or satellite images (i.e. ee.ImageCollection). To do so we create a function that has one argument (for the sake of simplicity named ‘feature’ for features and ‘image’ for images) that calls the ‘.map’ function, to apply another function (e.g. in the case of buffering, ‘.buffer’) to every feature/image in the collection. In our case we have done this to buffer, mask clouds, mask water and calculate NDVI. For the cloud mask function we use the ‘ee.Algorithms.Landsat.simpleCloudScore’ functions which takes input from beightness, temperature and NDSI and provides a score from 1-100 on liklihood of cloud (we use a threshold of below 25 using the ‘.lt’ function). A similar procedure is used to mask water pixels, where we only keep image pixels that equal 1 (as derived from the Hansen image). For the NDVI, we are using a function called ‘normalizedDifference’ which takes the Near Infra-red band (B4) as the first argument and the red band (B3) as the second argument.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iterating&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Iterating&lt;/h4&gt;
&lt;p&gt;If you need to calculate cumulative measures then mapping won’t work because it applies a function to each image independently, in this case you need to use &lt;a href=&#34;https://developers.google.com/earth-engine/ic_iterating&#34;&gt;‘.iterate’&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reducing&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Reducing&lt;/h4&gt;
&lt;p&gt;Reducing it synonymous with aggregating the data and can be done using the ‘.reduce’ function. In our case I’ve first reduced all the images to one image by calculating the median of each pixel using ‘ee.Reducer.median’. We then apply the same function by over our regions (i.e. datazones), setting the scale (determined by the resolution of the image).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;printing&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Printing&lt;/h4&gt;
&lt;p&gt;If you want to check that a function is running correctly, or that something has loaded, or to draw a chart, the ‘print’ command is needed. The output will appear in the console.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualising&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Visualising&lt;/h4&gt;
&lt;p&gt;Importantly when visualising to the map any commands will start with ‘Map.’. The ‘.addlayer’ function will add the image tothe screen, with a number of options available for how it looks (e.g. ‘.setCenter’). In our case we want to see the clipped NDVI layer for each population weighted datazone centroid:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/GEE_images/greenmap.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exporting&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Exporting&lt;/h4&gt;
&lt;p&gt;Importantly when exporting any commands will start with ‘Export.’. GEE is linked to your google drive so export using the ‘.table.toDrive’ function is very easy. In our case we have done some cleaning (taken out a redundant column and added an important one showing the year). The ourput is added to the Task Manager, where you need to manually click run to start the download to your drive.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cleaning&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Cleaning&lt;/h4&gt;
&lt;p&gt;The exported data can be cleaned in R. We first want to gather all the files for each year into a single file. Next we want to fill in the missing values. The missing values (~5% of which ~70% were in 2015) were imputed with a kalman smoothing using the ‘imputeTS’ package. Kalman smoothing was chosen as this (along with na.interpolation and na.seadec) will yield the best results (Moritz and Bartz-Beielstein, 2017) The NDVI values were then aggregated to datazone level and saved as csv files. They were also aggregated at a local authority level; here is the time series graph (loess smoothed line):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/GEE_images/LANDVI.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An interactive version can be accessed &lt;a href=&#34;http://rpubs.com/Marko/NDVI_LA&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;small&gt;
Gorelick, N., M. Hancher, M. Dixon, S. Ilyushchenko, D. Thau and R. Moore (2017). “Google Earth Engine: Planetary-scale geospatial analysis for everyone.” Remote Sensing of Environment 202: 18-27.&lt;/p&gt;
&lt;p&gt;Hansen, M. C., P. V. Potapov, R. Moore, M. Hancher, S. A. Turubanova, A. Tyukavina, D. Thau, S. V. Stehman, S. J. Goetz, T. R. Loveland, A. Kommareddy, A. Egorov, L. Chini, C. O. Justice and J. R. G. Townshend (2013). “High-Resolution Global Maps of 21st-Century Forest Cover Change.” Science 342(6160): 850-853.&lt;/p&gt;
&lt;p&gt;Klompmaker, J. O., G. Hoek, L. D. Bloemsma, U. Gehring, M. Strak, A. H. Wijga, C. van den Brink, B. Brunekreef, E. Lebret and N. A. H. Janssen (2018). “Green space definition affects associations of green space with overweight and physical activity.” Environ Res 160: 531-540.&lt;/p&gt;
&lt;p&gt;Moritz S and Bartz-Beielstein T (2017). imputeTS: Time Series Missing Value Imputation in R. The R Journal, 9(1), pp. 207-218. &lt;a href=&#34;https://journal.r-project.org/archive/2017/RJ-2017-009/index.html&#34; class=&#34;uri&#34;&gt;https://journal.r-project.org/archive/2017/RJ-2017-009/index.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Richardson, E. A., N. K. Shortt, R. Mitchell and J. Pearce (2018). “A sibling study of whether maternal exposure to different types of natural space is related to birthweight.” Int J Epidemiol 47(1): 146-155.&lt;/p&gt;
&lt;p&gt;&lt;/small&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;div id=&#34;javascript-code&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1. Javascript Code&lt;/h4&gt;
&lt;script src=&#34;https://gist.github.com/markocherrie/b7486f8e5eb1d9a82d095280f4ef7d0d.js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;another-gee-example&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;2. Another GEE example&lt;/h4&gt;
&lt;p&gt;I have lived in three places in Edinburgh during my life (childhood, undergrad. and postdoc.), and I wondered which had the highest greeness recently. We can manually set the coordinates and get the median greeness for each year from 2011-2018. The figure shows that using the daily images there is a lot of noise, but that a stable pattern arises over time (1st place childhood, 2nd place undergraduate, 3rd place postdoc):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/GEE_images/real.png&#34; /&gt;
But it looks like my ideal of living at the top of Ben Lomond is the clear winner:
&lt;img src=&#34;img/GEE_images/ideal.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-code&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;3. R code&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## export datazone as kml
library(rgdal)
wgs84 = &amp;#39;+proj=longlat +datum=WGS84&amp;#39;
DZcent&amp;lt;-readOGR(&amp;quot;boundaries/SG_DataZone_Cent_2011.shp&amp;quot;)
DZcent = spTransform(DZcent, CRS(wgs84))
writeOGR(DZcent, dsn=&amp;quot;boundaries/DZCent_2011.kml&amp;quot;, layer= &amp;quot;DZcent&amp;quot;, driver=&amp;quot;KML&amp;quot;, dataset_options=c(&amp;quot;NameField=name&amp;quot;))

# create on file from individual years
files  &amp;lt;- list.files(pattern = &amp;#39;\\.csv&amp;#39;)
tables &amp;lt;- lapply(files, read.csv, header = TRUE)
combined.df &amp;lt;- do.call(rbind , tables)
DZ_NDVI&amp;lt;-subset(combined.df, select=c(&amp;quot;DataZone&amp;quot;, &amp;quot;Year&amp;quot;, &amp;quot;median&amp;quot;))
#write.csv(DZ_NDVI, &amp;quot;DZ_green.csv&amp;quot;, row.names=F)

# read data in
#setwd(&amp;quot;C:/Users/mcherrie/Google Drive/GEE_output&amp;quot;)
data&amp;lt;-read.csv(&amp;quot;DZ_green.csv&amp;quot;)

# Missing
table(is.na(data$median))
# 6004/112588*100 (5% missing)

# impute missing
library(imputeTS)
# levels here is the name of the datazone
levels&amp;lt;-levels(data$DataZone)[1:2]
# function to fill in missing for each datazone
tsinterp&amp;lt;-function(x){
x &amp;lt;- ts(x, frequency = 1)
y &amp;lt;- as.data.frame(na.kalman(x)) 
return(y)
}
# apply the function and collect results
data2&amp;lt;-tapply(data$median, data$DataZone, tsinterp)
data2&amp;lt;-data.frame(data2)
data3&amp;lt;-data.frame(t(data2))
colnames(data3) &amp;lt;- c(&amp;#39;2001&amp;#39;, &amp;#39;2002&amp;#39;, &amp;#39;2003&amp;#39;, &amp;#39;2004&amp;#39;, &amp;#39;2005&amp;#39;, &amp;#39;2006&amp;#39;, &amp;#39;2007&amp;#39;, &amp;#39;2008&amp;#39;, &amp;#39;2009&amp;#39;, &amp;#39;2010&amp;#39;, 
&amp;#39;2011&amp;#39;, &amp;#39;2012&amp;#39;, &amp;#39;2013&amp;#39;, &amp;#39;2014&amp;#39;, &amp;#39;2015&amp;#39;, &amp;#39;2016&amp;#39;, &amp;#39;2017&amp;#39;)
data3$DataZone&amp;lt;-rownames(data3)
data4&amp;lt;-data3 %&amp;gt;%
  gather(&amp;quot;Year&amp;quot;,&amp;quot;NDVI&amp;quot; ,1:17)

# add to the council names data 
lookup&amp;lt;-read.csv(&amp;quot;lookup.csv&amp;quot;)
DZ_data&amp;lt;-merge(data4, lookup, by=&amp;quot;DataZone&amp;quot;)
nameLA&amp;lt;-read.csv(&amp;quot;councilnames.csv&amp;quot;)
DZ_data&amp;lt;-merge(DZ_data, nameLA, by=&amp;quot;Council&amp;quot;)

# get stats by LA
library(dplyr)
LA_data&amp;lt;-DZ_data %&amp;gt;%
  group_by(.dots=c(&amp;quot;Year&amp;quot;,&amp;quot;Councilname&amp;quot;)) %&amp;gt;%
  summarise(NDVI=median(NDVI))

## get plotly graph

library(ggplot2)
p&amp;lt;-ggplot(LA_data,aes(x=Year,y=NDVI,colour=Councilname,group=Councilname)) + geom_smooth(se = FALSE)+
  geom_point()
library(plotly)
ggplotly(p)

# create datazone data
master&amp;lt;-NULL
for(i in levels){
y&amp;lt;-read.csv(&amp;quot;DZ_green.csv&amp;quot;)
y&amp;lt;-subset(data, DataZone==i)
x &amp;lt;- ts(y$median, frequency = 1)
y &amp;lt;- as.data.frame(na.kalman(x))
master&amp;lt;-rbind(y, master)
}

# add to the geography lookup
lookup&amp;lt;-read.csv(&amp;quot;lookup.csv&amp;quot;)
data_admin&amp;lt;-merge(data, lookup, by=&amp;quot;DataZone&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;script src=&#34;//yihui.name/js/math-code.js&#34;&gt;&lt;/script&gt;
&lt;script async
src=&#34;//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
